runOnAllTags: &runOnAllTags
  filters:
    tags:
      only: /.*/

# This runs a command only on rc builds,
# which is a tagged build with a tag matching the below regex.
runOnlyOnReleaseCandidateBuilds: &runOnlyOnReleaseCandidateBuilds
  filters:
    tags:
      only: /^\d+\.\d+\.\d+\.\d+-rc\.\d$/
    branches:
      ignore: /.*/

# This runs a command only on release builds,
# which is a tagged build with a tag matching the below regex.
runOnlyOnReleaseBuilds: &runOnlyOnReleaseBuilds
  filters:
    tags:
      only: /^\d+\.\d+\.\d+\.\d+$/
    branches:
      ignore: /.*/

runOnAllTagsWithDockerIOPullCtx: &runOnAllTagsWithDockerIOPullCtx
  <<: *runOnAllTags
  context: docker-io-pull

runOnAllTagsWithDockerIOPushCtx: &runOnAllTagsWithDockerIOPushCtx
  <<: *runOnAllTags
  context: docker-io-push

setupGoogleAppCreds: &setupGoogleAppCreds
  run:
    name: Setup GCloud Service Account
    command: |
      touch /tmp/gcp.json
      chmod 0600 /tmp/gcp.json
      echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" >/tmp/gcp.json
      cci-export GOOGLE_APPLICATION_CREDENTIALS /tmp/gcp.json
      gcloud auth activate-service-account --key-file /tmp/gcp.json
      gcloud auth list

loginToGCR: &loginToGCR
  run:
    name: Docker login to gcr.io
    command: docker login -u _json_key -p "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" https://gcr.io
    when: always

restoreGoBuildCache: &restoreGoBuildCache
  restore_cache:
    name: Restoring Go build cache
    keys:
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-master-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-
      - go-cache-v1-

saveGoBuildCache: &saveGoBuildCache
  save_cache:
    name: Saving Go build cache
    key: go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /home/circleci/.cache/go-build

goModCacheKey: &goModCacheKey 'rox-go-pkg-mod-v2-{{ checksum "go.sum" }}'
restoreGoModCache: &restoreGoModCache
  restore_cache:
    name: Restore Go module cache
    keys:
      - *goModCacheKey

saveGoModCache: &saveGoModCache
  save_cache:
    name: Saving Go module cache
    key: *goModCacheKey
    paths:
      - /go/pkg/mod

restoreGoBuildBinaryCache: &restoreGoBuildBinaryCache
  restore_cache:
    name: Restoring Go binary build cache
    keys:
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-master-
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-
      - go-binary-cache-v2-

saveGoBuildBinaryCache: &saveGoBuildBinaryCache
  save_cache:
    name: Saving Go binary build cache
    key: go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /home/circleci/go/src/github.com/stackrox/rox/linux-gocache

goModBinaryCacheKey: &goModBinaryCacheKey 'rox-go-binary-pkg-mod-v2-{{ checksum "go.sum" }}'
restoreGoModBinaryCache: &restoreGoModBinaryCache
  restore_cache:
    name: Restore Go module cache
    keys:
      - *goModBinaryCacheKey

saveGoModBinaryCache: &saveGoModBinaryCache
  save_cache:
    name: Saving Go module cache
    key: *goModBinaryCacheKey
    paths:
      - /home/circleci/go/pkg/mod

gradleCacheKey: &gradleCacheKey 'v1-rox-gradle-{{ checksum "qa-tests-backend/build.gradle" }}'
restoreGradle: &restoreGradle
  restore_cache:
    keys:
      - *gradleCacheKey
      - v1-rox-gradle

getECRDockerPullPassword: &getECRDockerPullPassword
  run:
    name: Get AWS ECR Docker Pull Password
    command: |
      set +e
      aws --version
      PASS="$(aws --region="${AWS_ECR_REGISTRY_REGION}" ecr get-login-password)"
      cci-export AWS_ECR_DOCKER_PULL_PASSWORD "${PASS}"
      exit 0

refreshAlpineBaseImage: &refreshAlpineBaseImage
  run:
    name: Refresh base image
    command: docker pull alpine:3.11

setupRoxctl: &setupRoxctl
  run:
    name: Setup Roxctl from bin
    command: |
      cp bin/linux/roxctl $GOPATH/bin/roxctl

setupLicense: &setupLicense
  run:
    name: Creating License Key
    command: |
      if [[ -n "$CIRCLE_TAG" ]] || .circleci/pr_has_label.sh "ci-release-build"; then
        echo "Issuing a CI license for testing the release build"
        ROX_LICENSE_KEY="$(licenses/ci.sh)"
      else
        echo "Using the development license for CI"
        ROX_LICENSE_KEY="$(cat deploy/common/dev-license.lic)"
      fi
      cci-export ROX_LICENSE_KEY "$ROX_LICENSE_KEY"

setupDefaultTLSCerts: &setupDefaultTLSCerts
  run:
    name: Setup default TLS certificates
    command: |
      cert_dir="$(mktemp -d)"
      ./tests/scripts/setup-certs.sh "$cert_dir" custom-tls-cert.central.stackrox.local "Server CA"
      cci-export ROX_DEFAULT_TLS_CERT_FILE "${cert_dir}/tls.crt"
      cci-export ROX_DEFAULT_TLS_KEY_FILE "${cert_dir}/tls.key"
      cci-export DEFAULT_CA_FILE "${cert_dir}/ca.crt"
      cci-export ROX_TEST_CA_PEM "$(cat "${cert_dir}/ca.crt")"
      cci-export ROX_TEST_CENTRAL_CN "custom-tls-cert.central.stackrox.local"

      cci-export TRUSTSTORE_PATH "${cert_dir}/keystore.p12"

      echo "Contents of ${cert_dir}:"
      ls -al "${cert_dir}"

setupClientTLSCerts: &setupClientTLSCerts
  run:
    name: Setup client auth TLS certificates
    command: |
      cert_dir="$(mktemp -d)"
      ./tests/scripts/setup-certs.sh "$cert_dir" "Client Certificate User" "Client CA"

      cci-export KEYSTORE_PATH "${cert_dir}/keystore.p12"
      cci-export CLIENT_CA_PATH "${cert_dir}/ca.crt"
      cci-export CLIENT_CERT_PATH "${cert_dir}/tls.crt"
      cci-export CLIENT_KEY_PATH "${cert_dir}/tls.key"

bounceCentral: &bounceCentral
  run:
    name: "Bounce Central"
    command: |
      kubectl -n stackrox delete po $(kubectl -n stackrox get po -l app=central -o=jsonpath='{.items[0].metadata.name}') --grace-period=0

waitForAPI: &waitForAPI
  run:
    name: Wait for the API server to be up
    command: |
      # This script cannot be refactored out in our scripts/ directory because we checkout older commits
      # as a part of tests and this function is widely used

      echo "Waiting for central to start"
      start_time="$(date '+%s')"
      while true; do
        central_json="$(kubectl -n stackrox get deploy/central -o json)"
        if [[ "$(echo "${central_json}" | jq '.status.replicas')" == 1 && "$(echo "${central_json}" | jq '.status.readyReplicas')" == 1 ]]; then
          break
        fi
        echo "$central_json"
        if (( $(date '+%s') - start_time > 300 )); then
          kubectl -n stackrox get pod -o wide
          kubectl -n stackrox get deploy -o wide
          echo >&2 "Timed out after 5m"
          exit 1
        fi
        echo -n .
        sleep 1
      done
      echo "central is running"

      export API_HOSTNAME=localhost
      export API_PORT=8000
      if [[ "${LOAD_BALANCER}" == "lb" ]]; then
        export API_HOSTNAME=$(./scripts/k8s/get-lb-ip.sh)
        export API_PORT=443
      fi
      export API_ENDPOINT="${API_HOSTNAME}:${API_PORT}"
      export METADATA_URL="https://${API_ENDPOINT}/v1/metadata"
      echo "METADATA_URL is set to ${METADATA_URL}"
      set +e
      NUM_SUCCESSES_IN_A_ROW=0
      SUCCESSES_NEEDED_IN_A_ROW=3
      for i in $(seq 1 40); do
        metadata="$(curl -sk --connect-timeout 5 --max-time 10 "${METADATA_URL}")"
        if [[ $? -eq 0 && "$(jq '.licenseStatus' -r \<<<"$metadata")" != "RESTARTING" ]]; then
          NUM_SUCCESSES_IN_A_ROW=$(( NUM_SUCCESSES_IN_A_ROW + 1 ))
          if [[ "${NUM_SUCCESSES_IN_A_ROW}" == "${SUCCESSES_NEEDED_IN_A_ROW}" ]]; then
            break
          fi
          sleep 2
          continue
        fi
        NUM_SUCCESSES_IN_A_ROW=0
        sleep 5
      done
      if [[ "${NUM_SUCCESSES_IN_A_ROW}" != "${SUCCESSES_NEEDED_IN_A_ROW}" ]]; then
        kubectl -n stackrox get pod
        echo "Failed to connect to Central. Failed with ${NUM_SUCCESSES_IN_A_ROW} successes in a row"
        exit 1
      fi
      cci-export API_HOSTNAME "$API_HOSTNAME"
      cci-export API_PORT "$API_PORT"
      cci-export API_ENDPOINT "${API_ENDPOINT}"

waitForSensorK8s: &waitForSensorK8s
  run:
    name: Wait for the Sensor to be running K8s
    command: |
      ./scripts/ci/sensor-wait.sh

determineWhetherToRunUIDevServer: &determineWhetherToRunUIDevServer
  run:
    name: Determine whether to run the UI dev server
    command: |
      if .circleci/pr_has_label.sh ci-ui-e2e-coverage; then
        echo "Running UI E2Es against the dev server with code instrumentation due to the presence of the ci-ui-e2e-coverage label."
        # it's a dev server, so all logic to run dev server still applies
        cci-export RUN_UI_DEV_SERVER true
        cci-export UI_E2E_COLLECT_COVERAGE true
      elif .circleci/pr_has_label.sh ci-ui-dev-server; then
        echo "Running UI E2Es against the dev server due to the presence of the ci-ui-dev-server label."
        cci-export RUN_UI_DEV_SERVER true
      else
        echo "Running UI E2Es against the production server. Apply the ci-ui-dev-server label to your PR to run against the dev server."
        cci-export RUN_UI_DEV_SERVER false
      fi

runUIDevServer: &runUIDevServer
  run:
    name: Run the UI dev server
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        export YARN_START_TARGET="https://${API_ENDPOINT}"
        if [ "${UI_E2E_COLLECT_COVERAGE}" = "true" ]; then
          make -C ui start-coverage
        else
          make -C ui start
        fi
      fi
    background: true

waitForUIDevServer: &waitForUIDevServer
  run:
    name: Wait for the UI dev server to be up
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
        curl -k --max-time 60 --retry 50 --retry-connrefused -4 --retry-delay 5 --retry-max-time 300 https://localhost:3000/
      else
        echo "Not running the dev server, skipping..."
      fi

runUIE2E: &runUIE2E
  run:
    name: UI e2e tests
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        export UI_BASE_URL="https://localhost:3000"
      else
        if [[ "${LOAD_BALANCER}" == "lb" ]]; then
          sudo sh -c "echo >>/etc/hosts ${API_HOSTNAME} central-lb"
          export UI_BASE_URL="https://central-lb:443"
        else
          export UI_BASE_URL="https://localhost:${LOCAL_PORT}"
        fi
      fi

      if [ "${UI_E2E_COLLECT_COVERAGE}" = "true" ]; then
        make -C ui test-e2e-coverage
      else
        make -C ui test-e2e
      fi

collectImbuedUILogs: &collectImbuedUILogs
  run:
    name: Collect imbued UI logs
    command: |
      mkdir -p /tmp/imbued-ui-logs
      curl_cmd=(curl)
      if [[ -n "$ROX_USERNAME" && -n "$ROX_PASSWORD" ]]; then
        curl_cmd+=(-u "${ROX_USERNAME}:${ROX_PASSWORD}")
      fi
      status_code="$("${curl_cmd[@]}" -sk -o logs.zip -w "%{http_code}\n" "https://${API_ENDPOINT}/api/logimbue")"
      # Central returns 204 if there are no imbued logs
      if [ "${status_code}" -eq 204 ]; then
        echo "No imbued logs found."
      elif [ "${status_code}" -eq 200 ]; then
        echo "Logs imbued from the UI were found. Please find them in the artifacts section."
        unzip logs.zip -d /tmp/imbued-ui-logs
      else
        echo "Received error status code ${status_code} from the log imbue endpoint; is the log imbue handler broken?"
        exit 1
      fi
    when: always

storeImbuedUILogs: &storeImbuedUILogs
  store_artifacts:
    path: /tmp/imbued-ui-logs
    destination: imbued-ui-logs

collectK8sLogs: &collectK8sLogs
  run:
    name: Collect k8s logs

    command: |
      set +e
      ./scripts/ci/collect-service-logs.sh stackrox
      ./scripts/ci/collect-service-logs.sh kube-system
      ./scripts/ci/collect-service-logs.sh proxies
      ./scripts/ci/collect-service-logs.sh squid
      ./scripts/ci/collect-qa-service-logs.sh
      ./scripts/ci/collect-infrastructure-logs.sh
    when: always

connectToMonitoring: &connectToMonitoring
  run:
    name: Connect to monitoring via port-forward on 9443

    command: |
      set +e
      nohup kubectl port-forward -n 'stackrox' svc/monitoring "9443:8443" 1>/dev/null 2>&1 &
      sleep 5
    when: always

collectMonitoringImages: &collectMonitoringImages
  run:
    name: Collect monitoring images

    command: |
      set +e
      ./monitoring/grafana/fetch-core-images.sh
    when: always

storeMonitoringImages: &storeMonitoringImages
  store_artifacts:
    path: monitoring/grafana/captures
    destination: monitoring-images

storeMonitoringMetrics: &storeMonitoringMetrics
  store_artifacts:
    path: monitoring/grafana/rawmetrics.zip
    destination: monitoring-metrics.zip

benchmarkDashboard: &benchmarkDashboard
  run:
    name: Benchmark dashboard queries

    command: |
      ./scripts/ci/bench-and-graph.sh .*Dashboard ./scale/tests/

storeDashBenchImages: &storeDashBenchImages
  store_artifacts:
    path: monitoring/grafana/captures
    destination: monitoring-images-dashboard-bench

benchmarkCompliance: &benchmarkCompliance
  run:
    name: Benchmark compliance queries

    command: |
      ./scale/mocksensor/launch_mock_sensor.sh scale-test --node-interval=10ms --max-nodes=500 --deployment-interval=10ms --max-deployments=1000
      # allow enough time for all the deployments to be processed
      sleep 400
      ./scripts/ci/bench-and-graph.sh .*Compliance ./scale/tests/

storeComplianceBenchImages: &storeComplianceBenchImages
  store_artifacts:
    path: monitoring/grafana/captures
    destination: monitoring-images-compliance-bench

benchmarkDryRunPolicies: &benchmarkDryRunPolicies
  run:
    name: Benchmark dry run policies

    command: |
      ./scripts/ci/bench-and-graph.sh .*DryRunPolicies ./scale/tests

storeDryRunBenchImages: &storeDryRunBenchImages
  store_artifacts:
    path: monitoring/grafana/captures
    destination: monitoring-images-dryrun-bench

storeK8sLogs: &storeK8sLogs
  store_artifacts:
    path: /tmp/k8s-service-logs
    destination: k8s-service-logs

storeQALogs: &storeQALogs
  store_artifacts:
    path: /tmp/qa-tests-backend-logs
    destination: qa-tests-backend-logs

storeProfilingResults: &storeProfilingResults
  store_artifacts:
    path: /tmp/pprof.zip
    destination: pprof.zip

storeUITestReports: &storeUITestReports
  store_test_results:
    path: ui/test-results/reports

storeUITestArtifacts: &storeUITestArtifacts
  store_artifacts:
    path: ui/test-results/artifacts
    destination: ui-test-artifacts

storeQATestResults: &storeQATestResults
  store_test_results:
    path: qa-tests-backend/build/test-results/test

storeQASpockReports: &storeQASpockReports
  store_artifacts:
    path: ./qa-tests-backend/build/spock-reports
    destination: qa-test-report

backupDB: &backupDB
  run:
    name: Backup DB
    command: |
      mkdir -p db-backup
      roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db backup --output db-backup

storeDBBackupArtifact: &storeDBBackupArtifact
  store_artifacts:
    path: db-backup
    destination: db-backup

updateTestRail: &updateTestRail
  run:
    name: Update test results in TestRail
    command: |
      if [[ -z "${CIRCLE_TAG}" ]] || .circleci/is-nightly-tag.sh; then
        echo "Only run TestRail updater on tagged release/RC builds... skipping."
        exit 0
      fi

      ls -la qa-tests-backend/gradle-results
      export RESULTS_FILE_PATH=$(pwd)/qa-tests-backend/gradle-results
      export CI_JOB_NAME=${CIRCLE_JOB}
      export IMAGE_TAG=${CIRCLE_TAG}
      echo $RESULTS_FILE_PATH
      echo $CI_JOB_NAME
      echo $IMAGE_TAG
      make -C qa-tests-backend update-test-rail
    when: always

getPrometheusMetricParser: &getPrometheusMetricParser
  run:
    name: Get prometheus-metric-parser
    command: |
      export GOPRIVATE=github.com/stackrox
      go get github.com/stackrox/prometheus-metric-parser
      prometheus-metric-parser help

version: 2.1

parameters:
  # A block of external parameters used to run qa-backend-tests against an image set
  # released via a non-standard pipeline. e.g. DSOP images. See: designated-image-tests.
  run_designated_image_test:
    type: boolean
    default: false
  main_image_tag:
    type: string
    default: ""
  main_image_repo:
    type: string
    default: ""
  registry_username:
    type: string
    default: ""
  roxctl_image_repo:
    type: string
    default: ""
  scanner_image:
    type: string
    default: ""
  registry_password:
    type: string
    default: ""
  collector_image_repo:
    type: string
    default: ""
  scanner_db_image:
    type: string
    default: ""
  rox_license_key:
    type: string
    default: ""

orbs:
  win: circleci/windows@2.2.0

executors:
  custom:
    docker:
      - image: docker.io/stackrox/apollo-ci:0.3.5
        auth:
          username: $DOCKER_IO_PULL_USERNAME
          password: $DOCKER_IO_PULL_PASSWORD
    working_directory: /go/src/github.com/stackrox/rox

  binary-builder:
    docker:
      - image: docker.io/stackrox/main:testing-builder-ci
        auth:
          username: $DOCKER_IO_PULL_USERNAME
          password: $DOCKER_IO_PULL_PASSWORD
    working_directory: /go/src/github.com/stackrox/rox

commands:
  build-liveness-check:
    description: "Ensure workflow is still live"
    steps:
      - setup-gcp
      - run:
          name: Ensure workflow is still live
          command: .circleci/check-workflow-live.sh

  check-label-to-skip-tests:
    description: "Checks GitHub for the given label, and skips the tests if we're on a PR, and the label is present."
    parameters:
      label:
        type: string
    steps:
      - run:
          name: Determine whether to skip tests
          command: |
            if .circleci/pr_has_label.sh << parameters.label >> ; then
              echo "Skipping tests because of the presence of the << parameters.label >> label..."
              circleci step halt
            fi

  validate-upgrade:
    description: "Validates the upgrade was successful against upgrade tests"
    parameters:
      upgrade-cluster-id:
        type: string
      desc:
        type: string
    steps:
      - run:
          name: "<< parameters.desc >>"
          command: |
            export CLUSTER=K8S
            cci-export UPGRADE_CLUSTER_ID << parameters.upgrade-cluster-id >>
            make -C qa-tests-backend upgrade-test

  validate-upgrade-and-check-logs-with-bouncing:
    description: "Validates the upgrade was successful against upgrade tests, bounces Central, then repeats"
    parameters:
      upgrade-cluster-id:
        type: string
      desc:
        type: string
    steps:
      - validate-upgrade:
          upgrade-cluster-id: "<< parameters.upgrade-cluster-id >>"
          desc: "<< parameters.desc >> (pre bounce)"
      - check-stackrox-logs:
          collect: true
      - *bounceCentral
      - *waitForAPI
      - validate-upgrade:
          upgrade-cluster-id: "<< parameters.upgrade-cluster-id >>"
          desc: "<< parameters.desc >> (post bounce)"
      - check-stackrox-logs:
          collect: true

  check-label-exists-or-skip:
    description: "Checks GitHub for the given label, and skips the test if we're on a PR and the label is NOT present."
    parameters:
      label:
        type: string
      run-on-master:
        type: boolean
        default: true
    steps:
      - when:
          condition: << parameters.label >>
          steps:
            - run:
                name: Determine whether to run tests
                command: |
                  set +e
                  if [[ "${CIRCLE_BRANCH}" == "master" && "<< parameters.run-on-master >>" != "true" ]]; then
                     echo "Skipping tests because we're on master, but this test is not on master by default."
                     circleci step halt
                  elif [[ -z "${CIRCLE_TAG}" ]]; then
                    .circleci/pr_has_label.sh "<< parameters.label >>"
                    if [ $? -eq 1 ]; then
                      echo "Skipping tests because we're on a PR. Apply the << parameters.label >> label to your PR if you want to run them."
                      circleci step halt
                    fi
                  fi

  check-master-or-tag-or-label:
    description: "If run-on-master is set, run on master.  If run-on-tags is set, run on all tags.  If the label is set, run when the label is present.  Otherwise skip."
    parameters:
      label:
        type: string
      run-on-master:
        type: boolean
      run-on-tags:
        type: boolean
    steps:
      - when:
          condition: << parameters.label >>
          steps:
            - run:
                name: Determine whether to run tests
                command: |
                  set +e
                  halt=1
                  if [[ "<< parameters.run-on-master >>" == "true" && "${CIRCLE_BRANCH}" == "master" ]]; then
                    echo "Running tests because we are on master"
                    halt=0
                  elif [[ "<< parameters.run-on-tags >>" == "true" && -n "${CIRCLE_TAG}" ]]; then
                    echo "Running tests because this is a tagged build"
                    halt=0
                  elif [[ -n "<< parameters.label >>" ]]; then
                    # Split on commas
                    IFS=', ' ;for label in `echo "<< parameters.label >>"`; do
                      .circleci/pr_has_label.sh "${label}"
                      if [[ $? -eq 0 ]]; then
                        echo "Running tests because the label << parameters.label >> has been set"
                        halt=0
                      fi
                    done
                  fi

                  if [[ "${halt}" -eq 1 ]]; then
                    echo "Not running tests"
                    circleci step halt
                  fi

  get-and-store-debug-dump:
    description: Runs roxctl debug dump and stores it for the job.
    steps:
      - run:
          name: Pull the dump from Central
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central debug dump --output-dir debug-dump
            ls -l debug-dump
      - store_artifacts:
          path: debug-dump
          destination: debug

  pull-stackrox-api-data:
    description: Pull data from the StackRox API.
    steps:
      - run:
          name: Pull data from the StackRox API
          command: |
            ./scripts/grab-data-from-central.sh stackrox-api
            ls -l stackrox-api
      - store_artifacts:
          path: stackrox-api
          destination: stackrox-api

  check-stackrox-logs:
    parameters:
      collect:
        # In most jobs the logs are already grabbed by collectK8sLogs
        type: boolean
        default: false
      exclude-in-restart-check:
        # A regular expression to filter out logs for the restart check
        type: string
        default: "check-them-all"
      exclude-in-error-check:
        # A regular expression to filter out logs for the suspicious entry check
        type: string
        default: "check-them-all"

    description: Checks StackRox logs to ensure all services operated normally
    steps:
      - when:
          condition: << parameters.collect >>
          steps:
            - run:
                name: Collect the StackRox logs
                command: |
                  ./scripts/ci/collect-service-logs.sh stackrox

      - run:
          name: Verify that StackRox services did not restart during the runs
          command: |
            if [[ ! -d "/tmp/k8s-service-logs/stackrox/pods" ]]; then
              echo >&2 "StackRox logs were not collected. (Use collect: true or collectK8sLogs.)"
              exit 1
            fi
            previous_logs=$(ls /tmp/k8s-service-logs/stackrox/pods/*-previous.log || true)
            if [[ -n "${previous_logs}" ]]; then
                echo >&2 "Previous logs found: ${previous_logs}"
                filtered=$(ls ${previous_logs} | egrep -v "<< parameters.exclude-in-restart-check >>" || true)
                if [[ -n "${filtered}" ]]; then
                    echo >&2 "Reduced to: ${filtered}"
                    exit 1
                fi
            fi
          when: always

      - run:
          name: Verify that StackRox service logs contain no suspicious entries
          command: |
            if [[ ! -d "/tmp/k8s-service-logs/stackrox/pods" ]]; then
              echo >&2 "StackRox logs were not collected. (Use collect: true or collectK8sLogs.)"
              exit 1
            fi
            logs=$(ls /tmp/k8s-service-logs/stackrox/pods/*.log)
            filtered=$(ls ${logs} | egrep -v "<< parameters.exclude-in-error-check >>" || true)
            if [[ -n "${filtered}" ]]; then
                if ! scripts/ci/logcheck/check.sh ${filtered}; then
                    echo >&2 "Found at least one suspicious log file entry."
                    exit 1
                fi
            fi
          when: always

  install-ossls:
    steps:
      - run:
          name: Install ossls
          working_directory: /tmp
          command: |
            wget --quiet https://github.com/gruntwork-io/fetch/releases/download/v0.3.5/fetch_linux_amd64
            sudo install fetch_linux_amd64 /usr/bin/fetch
            export GITHUB_OAUTH_TOKEN="$GITHUB_TOKEN"
            fetch --repo="https://github.com/stackrox/ossls" --tag="0.10.0" --release-asset="ossls_linux_amd64" .
            sudo install ossls_linux_amd64 /usr/bin/ossls
            ossls version

  setup-egress-proxies:
    steps:
      - run:
          name: Set up egress proxies (if desired)
          command: |
            if ! .circleci/pr_has_label.sh ci-rox-egress-proxies; then
              echo "Not setting up egress proxies"
              exit 0
            fi

            kubectl -n squid apply -R -f tests/proxy/deployment/ || true  # might fail because of SCC
            configs=(tests/proxy/config/*.yaml)

            selected_config="${configs[$((RANDOM % ${#configs[@]}))]}"
            echo "Selected configuration file ${selected_config}"
            kubectl -n stackrox apply -f "${selected_config}"

            echo "Waiting for 3min for proxy config to be picked up"
            for i in {1..180}; do
              echo -n .
              sleep 1
            done
            echo

            if [[ "$CIRCLE_JOB" == "openshift-crio-api-e2e-tests" ]]; then
              echo "Skipping egress isolation on CRI-O due to network policy issues."
              exit 0
            fi

            echo "Isolating pods"
            kubectl -n stackrox apply -R -f tests/proxy/netpol/

  setup-go-build-env:
    steps:
      - run:
          name: Setup Go build environment
          command: |
            if .circleci/pr_has_label.sh "ci-release-build"; then
              cci-export GOTAGS release
            fi

  setup-dep-env:
    parameters:
      docker-login:
        type: boolean
        default: true

      use-main-rhel:
        type: boolean
        default: false

      use-websocket:
        type: boolean
        default: false

    steps:
      - run:
          name: Setup deployment env
          command: |
            <<# parameters.docker-login >>
            docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"
            <</ parameters.docker-login >>
            <<# parameters.use-websocket >>
            cci-export CLUSTER_API_ENDPOINT "wss://central.stackrox:443"
            <</ parameters.use-websocket >>
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export GOOGLE_APPLICATION_CREDENTIALS /tmp/gcp.json
            if [[ "<< parameters.use-main-rhel >>" == "true" ]]; then
              cci-export MAIN_IMAGE_REPO stackrox/main-rhel
              cci-export COLLECTOR_IMAGE_REPO stackrox/collector-rhel
              cci-export SCANNER_IMAGE "stackrox/scanner-rhel:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"
              cci-export SCANNER_DB_IMAGE "stackrox/scanner-db-rhel:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"
            fi

  setup-gcp:
    steps:
      - run:
          name: Setup GCP env
          command: |
            if [[ "$(gcloud config get-value core/project 2>/dev/null)" == "stackrox-ci" ]]; then
              echo "Current project is already set to stackrox-ci. Assuming configuration already applied."
              exit 0
            fi
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list
            gcloud config set project stackrox-ci
            gcloud config set compute/region us-central1
            gcloud config unset compute/zone
            gcloud config set core/disable_prompts True

  create-gke:
    parameters:
      wait:
        type: boolean
        default: true

    steps:
      - run:
          name: Create GKE cluster
          command: |
            source .circleci/create-cluster.sh && create-cluster
            <<# parameters.wait >>
            wait-for-cluster
            <</ parameters.wait >>

  teardown-gke:
    steps:
      - run:
          name: Tear down GKE cluster
          command: |
            ./scripts/ci/cleanup-deployment.sh || true

            CLUSTER_NAME="${CLUSTER_NAME:-prevent-ci-${CIRCLE_JOB}}"
            gcloud container clusters delete "$CLUSTER_NAME" --async

          when: always

  cleanup-clusters-on-fail:
    steps:
      # This only runs if the build has passed, thus short-circuiting the steps below.
      # The reason "when: on_fail" is not sufficient is that attach_workspace seems to always
      # run, and unnecessarily takes up space.
      - run:
          name: Halt if the build has passed
          command: circleci step halt

      - run:
          name: Ensure GCloud is configured
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list

            gcloud config set project stackrox-ci
            gcloud config set compute/region us-central1
            gcloud config unset compute/zone
            gcloud config set core/disable_prompts True
          when: on_fail

      - run:
          name: Record step as fatal failure
          command: |
            gsutil cp - "gs://stackrox-ci-status/workflows/${CIRCLE_WORKFLOW_ID}/fatal-failures/${CIRCLE_JOB}" \
              </dev/null
          when: on_fail

      - run:
          name: Delete all GKE clusters for this CI run
          # The command is inline here instead of invoking a script so we can even run it
          # when checkout failed due to a force push.
          command: |
            echo "Deleting all GKE clusters for workflow $CIRCLE_WORKFLOW_ID"

            while IFS='' read -r line || [[ -n "$line" ]]; do
            	[[ -n "$line" ]] || continue

            	if [[ "$line" =~ ^([^[:space:]]+)[[:space:]]+([^[:space:]]+)$ ]]; then
            		cluster="${BASH_REMATCH[1]}"
            		zone="${BASH_REMATCH[2]}"
            	else
            		echo >&2 "Could not parse line $line"
            	fi

            	echo "Deleting cluster ${cluster} in zone ${zone} ..."
            	gcloud container clusters delete "$cluster" \
            		--async \
            		--project stackrox-ci \
            		--zone "$zone" \
            		--quiet || true

            done < <(
            	gcloud container clusters list \
            		--project stackrox-ci \
            		--filter "resourceLabels.stackrox-ci-workflow=${CIRCLE_WORKFLOW_ID}" \
            		--format 'csv[no-heading,separator=" "](name,zone)'
            	)

          when: on_fail

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *loginToGCR
      - run:
          name: Destroy EKS cluster
          command: |
            if [ -z "${CIRCLE_TAG}" ]; then
              if ! .circleci/pr_has_label.sh ci-eks-tests; then
                echo "EKS Cluster not created, skipping destroy."
                exit 0
              fi
            fi
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

      - run:
          name: Destroy Kops cluster
          command: |
            if [ -z "${CIRCLE_TAG}" ]; then
              if ! .circleci/pr_has_label.sh ci-kops-tests; then
                echo "Kops Cluster not created, skipping destroy."
                exit 0
              fi
            fi
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/kops:$KOPS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

  provision-gke-cluster:
    parameters:
      cluster-id:
        type: string
      num-nodes:
        type: integer
        default: 3
      machine-type:
        type: string
        default: e2-standard-4

    steps:
      - setup-gcp

      - run:
          name: Assign environment variables
          command: |
            CLUSTER_NAME="rox-ci-<< parameters.cluster-id >>-${CIRCLE_BUILD_NUM}"
            cci-export CLUSTER_NAME "$CLUSTER_NAME"
            echo "Assigned cluster name is $CLUSTER_NAME"

            NUM_NODES="<< parameters.num-nodes >>"
            cci-export NUM_NODES "$NUM_NODES"
            echo "Number of nodes for cluster is $NUM_NODES"

            MACHINE_TYPE="<< parameters.machine-type >>"
            cci-export MACHINE_TYPE "$MACHINE_TYPE"
            echo "Machine type is set as to $MACHINE_TYPE"

            GKE_RELEASE_CHANNEL="stable"
            if .circleci/pr_has_label.sh ci-gke-release-channel-rapid; then
              GKE_RELEASE_CHANNEL="rapid"
            elif .circleci/pr_has_label.sh ci-gke-release-channel-regular; then
              GKE_RELEASE_CHANNEL="regular"
            fi
            cci-export GKE_RELEASE_CHANNEL "$GKE_RELEASE_CHANNEL"
            echo "Using gke release channel: $GKE_RELEASE_CHANNEL"

      - create-gke:
          wait: false

      - run:
          name: Save cluster config
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            mkdir -p "$CONFIG_DIR"
            echo "$CLUSTER_NAME" >>"${CONFIG_DIR}/name"
            gcloud config get-value compute/zone >>"${CONFIG_DIR}/zone"
            cp .circleci/gke-refresh-token-forever.sh "${CONFIG_DIR}/refresh-token-forever.sh"

      - build-liveness-check

      - run:
          name: Tear down cluster upon failure
          command: |
            gcloud container clusters delete "$CLUSTER_NAME" --async
          when: on_fail

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - .ci-clusters/<< parameters.cluster-id >>

  deploy-sensor-via-upgrader-for-upgrade-tests:
    parameters:
      desc:
        type: string
      upgrade-process-id:
        type: string

    steps:
      - run:
          name: Deploy sensor via upgrader (<< parameters.desc >>)
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5

            ROX_UPGRADE_PROCESS_ID=<< parameters.upgrade-process-id >> \
                ROX_CENTRAL_ENDPOINT="${API_ENDPOINT}" \
                ROX_MTLS_CA_FILE="$(pwd)/sensor-remote-new/ca.pem" \
                ROX_MTLS_CERT_FILE="$(pwd)/sensor-remote-new/sensor-cert.pem" \
                ROX_MTLS_KEY_FILE="$(pwd)/sensor-remote-new/sensor-key.pem" \
                KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              ./bin/linux/upgrader -workflow roll-forward -local-bundle sensor-remote-new -kube-config kubectl

            kill "$proxy_pid"

      - *waitForSensorK8s

  attach-gke-cluster:
    parameters:
      cluster-id:
        type: string

    steps:
      - run:
          name: Restore config for << parameters.cluster-id >> cluster
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            export CLUSTER_NAME="$(cat "${CONFIG_DIR}/name")"
            [[ -n "$CLUSTER_NAME" ]]
            export ZONE="$(cat "${CONFIG_DIR}/zone")"
            [[ -n "$ZONE" ]]
            gcloud config set compute/zone "$ZONE"
            cmd=(gcloud container clusters get-credentials --project stackrox-ci --zone "$ZONE" "$CLUSTER_NAME")
            "${cmd[@]}"
            echo "Restored config for cluster ${CLUSTER_NAME}"
            cci-export CLUSTER_NAME "$CLUSTER_NAME"
            cci-export ZONE "$ZONE"
            echo
            echo "Run the following command to attach to the cluster:"
            echo
            printf " %q" "${cmd[@]}"
            echo

      - run:
          name: Running refresh token script in the background
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            if [[ ! -x "${CONFIG_DIR}/refresh-token-forever.sh" ]]; then
              exit 0
            fi
            "${CONFIG_DIR}/refresh-token-forever.sh"
          background: true

      - run:
          name: Waiting for << parameters.cluster-id >> cluster to stabilize
          command: |
            source .circleci/create-cluster.sh && wait-for-cluster

  remove-existing-stackrox-resources:
    steps:
      - run:
          name: Remove all the existing StackRox resources (they can exist when Circle restarts a container)
          command: |
            kubectl -n stackrox delete cm,deploy,ds,networkpolicy,secret,svc,serviceaccount,validatingwebhookconfiguration,pv,pvc,clusterrole,clusterrolebinding,role,rolebinding,psp -l "app.kubernetes.io/name=stackrox" --wait || true
            kubectl delete -R -f scripts/ci/psp --wait || true
            kubectl delete ns stackrox --wait || true
            helm uninstall monitoring || true
            helm uninstall central || true
            helm uninstall scanner || true
            helm uninstall sensor || true

  deploy-webhook-server:
    steps:
      - run:
          name: Deploy Webhook server
          command: |
            certs_dir="$(mktemp -d)"
            ./scripts/ci/create-webhookserver.sh "${certs_dir}"
            GENERIC_WEBHOOK_SERVER_CA_CONTENTS="$(cat "${certs_dir}/ca.crt")"
            cci-export GENERIC_WEBHOOK_SERVER_CA_CONTENTS "${GENERIC_WEBHOOK_SERVER_CA_CONTENTS}"

  deploy-default-psp:
    steps:
      - run:
          name: Deploy Default PSP for stackrox namespace
          command: |
            ./scripts/ci/create-default-psp.sh

  deploy-authz-plugin:
    steps:
      - run:
          name: Deploy Default Authorization Plugin
          command: |
            ./scripts/ci/create-scopedaccessserver.sh

  deploy-anchore:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Deploy Anchore Scanner
                command: |
                  ./scripts/ci/anchore/deploy.sh qa-anchore qa
                  cci-export ANCHORE_ENDPOINT "http://qa-anchore-engine-api.qa-anchore:8228"
                background: true

  teardown-anchore:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Teardown Anchore Scanner
                command: |
                  ./scripts/ci/anchore/teardown.sh qa-anchore

  deploy-clair:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Deploy Clair Scanner
                command: |
                  ./scripts/ci/clair/deploy.sh qa-clair
                  cci-export CLAIR_ENDPOINT "http://clairsvc.qa-clair:6060"
                background: true

  teardown-clair:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Teardown Clair Scanner
                command: |
                  ./scripts/ci/clair/teardown.sh qa-clair

  run-qa-tests:
    parameters:
      cluster-id:
        type: string

      determine-whether-to-run:
        type: steps

      use-main-rhel:
        type: boolean
        default: false

      sensor-deploy-flavor:
        type: string
        default: kubectl

      use-websocket:
        type: boolean
        default: false

    steps:
      - checkout
      - check-backend-changes

      - steps: << parameters.determine-whether-to-run >>

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - setup-dep-env:
          use-main-rhel: << parameters.use-main-rhel >>
          use-websocket: << parameters.use-websocket >>
      - attach-gke-cluster:
          cluster-id: << parameters.cluster-id >>
      - remove-existing-stackrox-resources
      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-anchore:
          cluster-id: << parameters.cluster-id >>
      - deploy-clair:
          cluster-id: << parameters.cluster-id >>

      - deploy-stackrox:
          sensor-deploy-flavor: << parameters.sensor-deploy-flavor >>

          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - deploy-default-psp
      - deploy-webhook-server
      - deploy-authz-plugin

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform Part 1
          command: |
            export CLUSTER=K8S
            if [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
              echo "On master, running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            elif .circleci/pr_has_label.sh ci-all-qa-tests; then
              echo "ci-all-qa-tests label was specified, so running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            else
              echo "On a PR branch, running BAT tests only..."
              make -C qa-tests-backend bat-test || touch FAIL
            fi

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *storeQATestResults
      - copy-gradle-files:
          target-dir: ${CIRCLE_JOB}-part1

      - run:
          name: QA Automation Platform Part 2
          command: |
            export CLUSTER=K8S
            make -C qa-tests-backend sensor-bounce-test

      - run:
          name: Test for failure in Part 1
          command: |
            [[ ! -f FAIL ]]

      - save_cache:
          key: *gradleCacheKey
          paths:
            - ~/.gradle/caches/

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: ${CIRCLE_JOB}-part2

      - *updateTestRail

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs
      - *storeQALogs
      - *connectToMonitoring
      - *collectMonitoringImages
      - *storeMonitoringImages
      - *storeMonitoringMetrics

      - teardown-anchore:
          cluster-id: << parameters.cluster-id >>
      - teardown-clair:
          cluster-id: << parameters.cluster-id >>

      - teardown-gke

  provision-openshift-cluster:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""
      crio:
        type: boolean
        default: false
      version:
        type: string

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - run:
          name: Install kubectl
          working_directory: /tmp
          command: |
            wget https://storage.googleapis.com/kubernetes-release/release/v1.11.2/bin/linux/amd64/kubectl
            sudo install kubectl /usr/bin

      - *loginToGCR
      - run:
          name: Create cloud resources
          command: |
            mkdir -p openshift
            set +e
            try=0
            max_attempts=2
            while [[ ${try} -lt ${max_attempts} ]]; do
                try=$((try + 1))
                echo "============= Openshift provision try ${try} ============="
                docker run --rm -t -v "$PWD/openshift:/well-known" \
                  -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
                  "gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >>" \
                  create \
                    --name="${CIRCLE_WORKFLOW_ID:0:7}<< parameters.suffix >>" \
                    --crio="<< parameters.crio >>" \
                    --creation-source=ci \
                    --gcp-project=stackrox-ci \
                    --dns-project=stackrox-ci \
                    --dns-zone=openshift-ci-rox-systems \
                    2>&1 | scripts/ci/monitor-output.sh 60
                rc=$?
                if [[ ${rc} == 0 ]]; then
                    echo "============= Openshift provision succeeded ============="
                    break
                fi
                echo "============= Openshift provision failed on try ${try} ============="
                if [[ ${try} -lt ${max_attempts} ]]; then
                    docker run --rm -t -v "$PWD/openshift:/well-known" \
                      -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
                      "gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >>" \
                      destroy
                fi
            done
            if [[ $rc != 0 ]]; then
                exit $rc
            fi
            set -e

      - store_artifacts:
          path: openshift
          destination: openshift

      - run:
          name: Configure Kubeconfig
          command: |
            ls -lh $PWD/openshift
            export KUBECONFIG="$PWD/openshift/artifacts/config"
            kubectl get nodes -o wide

      - store_artifacts:
          path: openshift
          destination: openshift

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Create alias for OpenShift data directory
                command: |
                  cp -r openshift/ openshift<< parameters.suffix >>/

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - openshift<< parameters.suffix >>

      - destroy-openshift-cluster:
          when: on_fail
          version: << parameters.version >>

  destroy-openshift-cluster:
    parameters:
      version:
        type: string
      when:
        type: enum
        enum: ["on_fail", "always"]

    steps:
      - run:
          name: Pack openshift volume
          when: << parameters.when >>
          command: |
            docker create -v /well-known --name openshift alpine:3.9 /bin/true
            docker cp openshift/artifacts openshift:/well-known/artifacts

      - *loginToGCR
      - run:
          name: Destroy Openshift cluster
          when: << parameters.when >>
          command: |
            docker run --rm -t \
              --volumes-from openshift \
              -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
              gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >> \
              destroy

  run-openshift-tests:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""
      version:
        type: string
      use-main-rhel:
        type: boolean
        default: false
      sensor-deploy-flavor:
        type: string
        default: kubectl

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - setup_remote_docker

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Copy openshift data directory to canonical location
                command: |
                  [[ ! -d openshift ]] || rm -rf openshift
                  cp -r "openshift<< parameters.suffix >>" openshift

      - run:
          name: Sanity check OpenShift Env
          command: |
            pwd
            ls -lh $PWD/openshift
            export KUBECONFIG=$PWD/openshift/artifacts/config
            SUCCESS=0
            for i in {1..30}; do
             if oc get nodes; then
               SUCCESS=1
               break
             fi
             echo "Failed sanity check OpenShift. Retrying in 5 seconds"
             sleep 5
            done
            if (( !SUCCESS )); then
              echo "Sanity check failed"
              exit 1
            fi

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/openshift/artifacts/config"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.2
            cci-export OPENSHIFT_HOST "$(cat openshift/artifacts/master)"
            cci-export ROX_IMAGE_REGISTRY docker.io
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"
            if [[ "<< parameters.use-main-rhel >>" == "true" ]]; then
              cci-export MAIN_IMAGE_REPO stackrox/main-rhel
              cci-export COLLECTOR_IMAGE_REPO stackrox/collector-rhel
              cci-export SCANNER_IMAGE "stackrox/scanner-rhel:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"
              cci-export SCANNER_DB_IMAGE "stackrox/scanner-db-rhel:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"
            fi

            ./scripts/ci/openshift-gcr-secrets.sh

      - remove-existing-stackrox-resources
      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          orchestrator-flavor: openshift
          sensor-deploy-flavor: << parameters.sensor-deploy-flavor >>
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - deploy-webhook-server
      - deploy-authz-plugin

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform Part 1
          command: |
            export CLUSTER=OPENSHIFT
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            if [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
              echo "On master, running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            elif .circleci/pr_has_label.sh ci-all-qa-tests; then
              echo "ci-all-qa-tests label was specified, so running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            else
              echo "On a PR branch, running BAT tests only..."
              make -C qa-tests-backend bat-test || touch FAIL
            fi

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data

      - *storeQATestResults
      - copy-gradle-files:
          target-dir: openshift-part1

      - run:
          name: QA Automation Platform Part 2
          command: |
            export CLUSTER=OPENSHIFT
            export API_HOSTNAME=localhost
            export API_PORT="${LOCAL_PORT}"
            make -C qa-tests-backend sensor-bounce-test

      - run:
          name: Check for failures in part 1
          command: |
            [[ ! -f FAIL ]]

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: openshift-part2

      - *updateTestRail

      - *backupDB
      - *storeDBBackupArtifact

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - check-stackrox-logs:
          exclude-in-restart-check: "compliance"
          exclude-in-error-check: "compliance-previous"
      - *storeK8sLogs
      - *storeQALogs

      - destroy-openshift-cluster:
          when: always
          version: << parameters.version >>

  deploy-stackrox:
    parameters:
      sensor-deploy-flavor:
        type: string
        default: kubectl
      orchestrator-flavor:
        type: string
        default: k8s
      require-cluster-admin:
        type: boolean
        default: false
      validate-autoupgrade-label:
        type: boolean
        default: false
      post-central-deploy-steps:
        type: steps
        default: []
      helm-from-repo:
        type: boolean
        default: false
      validate-sensor-bundle:
        type: boolean
        default: true

    steps:
      - run:
          name: Monitor cluster & deployment
          command: |
            scripts/ci/deployment-minder.sh
          background: true

      - run:
          name: Deploy central to remote cluster
          command: |
            # If we're running a race check, then set CGO_CHECK=true so the central script will deploy it with strict checks
            if .circleci/is-nightly-tag.sh || .circleci/pr_has_label.sh ci-race-tests; then
                cci-export CGO_CHECKS "true"
                cci-export IS_RACE_BUILD "true"
            fi

            # Use stackrox.io if helm-from-repo is true
            if [[ << parameters.helm-from-repo >> == "true" ]]; then
              cci-export MAIN_IMAGE_REPO "stackrox.io/main"
              cci-export REGISTRY_USERNAME "$STACKROX_IO_USERNAME"
              cci-export REGISTRY_PASSWORD "$STACKROX_IO_PASSWORD"
              docker login -u "$STACKROX_IO_USERNAME" -p "$STACKROX_IO_PASSWORD" stackrox.io
              docker login -u "$STACKROX_IO_USERNAME" -p "$STACKROX_IO_PASSWORD" collector.stackrox.io
            fi

            if [[ -z "${OUTPUT_FORMAT:-}" ]]; then
              if .circleci/pr_has_label.sh ci-helm-deploy; then
                export OUTPUT_FORMAT=helm
              fi
            fi

            deploy_dir="deploy/<< parameters.orchestrator-flavor >>"
            "${deploy_dir}/central.sh"

            source scripts/k8s/export-basic-auth-creds.sh "$deploy_dir"
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - steps: << parameters.post-central-deploy-steps >>

      - *waitForAPI

      - run:
          name: Snapshot Kubernetes Resources
          command: |
            <<# parameters.validate-autoupgrade-label >>
            scripts/ci/sensorbundle-label/list-resources.sh >/tmp/k8s-resources-pre-sensor
            <</ parameters.validate-autoupgrade-label >>
            exit 0

      - run:
          name: Deploy Sensor
          command: |
            if [[ "<< parameters.sensor-deploy-flavor >>" == "kubectl" ]]; then
              echo "Deploying sensor using kubectl ... "
              <<# parameters.require-cluster-admin >>
              kubectl create clusterrolebinding temporary-admin --clusterrole=cluster-admin --user circleci-rox@stackrox-ci.iam.gserviceaccount.com
              <</ parameters.require-cluster-admin >>

              deploy/<< parameters.orchestrator-flavor >>/sensor.sh
              <<# parameters.require-cluster-admin >>
              kubectl delete clusterrolebinding temporary-admin
              <</ parameters.require-cluster-admin >>
            else
              echo "Deploying sensor using helm ... "
              rox_auth_token="$(
              curl -sk -u "${ROX_USERNAME}:${ROX_PASSWORD}" "https://${API_ENDPOINT}/v1/apitokens/generate" \
                -X POST \
                -d '{"name": "helm", "role": "Sensor Creator"}' \
                | jq -r '.token // ""')"
              export ROX_API_TOKEN=${rox_auth_token}
              MAIN_TAG=$(make --quiet tag)
              extra_args=()
              if [[ << parameters.helm-from-repo >> == "true" ]]; then
                cd /go/src/github.com/stackrox
                git clone git@github.com:stackrox/helm-charts.git
                BASE_DIR="/go/src/github.com/stackrox/helm-charts"
                RENDERED_CHART_DIR="${BASE_DIR}/${MAIN_TAG}"
                if [[ -d "$RENDERED_CHART_DIR/sensor" ]]; then
                  RENDERED_CHART_DIR="$RENDERED_CHART_DIR/sensor"
                fi
                cd ${RENDERED_CHART_DIR}
                sed -i -e 's/  name:/  name: remote/g' values.yaml
                sed -i -e 's/    createService: false/    createService: true/g' values.yaml
                cat values.yaml
                ./scripts/setup.sh -f values.yaml -e ${API_ENDPOINT}
              else
                BASE_DIR="/go/src/github.com/stackrox/rox"
                RENDERED_CHART_DIR="/tmp/${MAIN_TAG}"
                make render-helm-yamls
                # Copy values.yaml into generated tmp dir, run setup.sh and helm install
                cd ${RENDERED_CHART_DIR}
                cp ${BASE_DIR}/qa-tests-backend/helm/values.<< parameters.orchestrator-flavor >>.yaml values.yaml
                ./scripts/setup.sh -f values.yaml -e ${API_ENDPOINT}
                if [[ -f feature-flag-values.yaml ]]; then
                    extra_args+=(-f feature-flag-values.yaml)
                fi
              fi
              helm install --namespace stackrox -f values.yaml sensor . "${extra_args[@]}"
              rm -rf "${RENDERED_CHART_DIR}"
            fi

      - when:
          condition: << parameters.validate-sensor-bundle >>
          steps:
            - run:
                name: Validate Sensor bundle (upgrader)
                command: |
                  if [[ "<< parameters.sensor-deploy-flavor >>" == "kubectl" ]]; then
                    kubectl proxy --port 28001 &
                    proxy_pid=$!
                    sleep 5
                    KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
                      bin/linux/upgrader \
                        -kube-config kubectl \
                        -local-bundle deploy/<< parameters.orchestrator-flavor >>/sensor-deploy \
                        -workflow validate-bundle
                    kill "$proxy_pid"
                  else
                    echo "Skipping for helm sensor deploy...  "
                  fi

      - *waitForSensorK8s

      - run:
          name: Bounce collectors to avoid restarts on initial module pull
          command: |
            kubectl -n stackrox delete pod -l app=collector --grace-period=0

      - *waitForSensorK8s

      - run:
          name: Verify all new resources have the auto-upgrade label
          command: |
            <<# parameters.validate-autoupgrade-label >>
            {
              scripts/ci/sensorbundle-label/list-resources.sh
              cat /tmp/k8s-resources-pre-sensor
            } | sort | uniq -u >/tmp/k8s-new-resources-post-sensor

            scripts/ci/sensorbundle-label/list-resources.sh 'auto-upgrade.stackrox.io/component=sensor' \
                >/tmp/k8s-sensor-labeled-resources

            {
              cat /tmp/k8s-new-resources-post-sensor
              cat /tmp/k8s-sensor-labeled-resources
              cat /tmp/k8s-sensor-labeled-resources
              cat scripts/ci/sensorbundle-label/ignorelist 2>/dev/null
              cat scripts/ci/sensorbundle-label/ignorelist 2>/dev/null
            } | sort | uniq -u >/tmp/k8s-new-resources-unlabeled

            if [[ -s /tmp/k8s-new-resources-unlabeled ]]; then
              echo >&2 "The following resources were created by the sensor bundle, but don't carry the sensor bundle label:"
              cat >&2 /tmp/k8s-new-resources-unlabeled
              exit 1
            fi
            <</ parameters.validate-autoupgrade-label >>
            exit 0

  check-backend-changes:
    steps:
      - run:
          name: Check if there were any backend changes
          command: |
            if [[ "$CIRCLE_BRANCH" == "master" || -n "$CIRCLE_TAG" || "$CIRCLE_BRANCH" =~ ^release/.* ]]; then
              exit 0  # Always run on master, tags, or release branches
            fi
            merge_base="$(git merge-base HEAD origin/master)"
            { git diff --name-only "${merge_base}" || echo "???" ; } | egrep -qv '^ui/' || circleci step halt

  copy-gradle-files:
    description: "Copy the gradle results files into a common directory to later parse by the TestRail updater"
    parameters:
      target-dir:
        type: string
    steps:
      - run:
          name: Copy gradle test result files
          command: |
            mkdir -p qa-tests-backend/gradle-results
            mv qa-tests-backend/build/reports/tests/test/classes/ qa-tests-backend/gradle-results/<< parameters.target-dir >> || true
          when: always

  compare-with-stored-metrics:
    parameters:
      cluster-flavor:
        type: string
      test:
        type: string
    steps:
      - run:
          name: Compare with stored metrics
          command: |
            set -x
            GS_PATH=gs://stackrox-ci-metrics/<< parameters.test >>/<< parameters.cluster-flavor >>
            COMPARE_WITH=$(gsutil ls "${GS_PATH}"/stackrox_debug\* | sort | tail -1)
            echo "Using ${COMPARE_WITH} as metrics for comparison"
            mkdir /tmp/metrics
            gsutil cp "${COMPARE_WITH}" /tmp/metrics
            COMPARE_WITH=$(ls -1 /tmp/metrics | sort | tail -1)
            THIS_RUN=$(echo ${PWD}/debug-dump/stackrox_debug*.zip)
            echo "Comparing with ${THIS_RUN}"
            COMPARE=${PWD}/scripts/ci/compare-debug-metrics.sh
            set +e
            cd /tmp/metrics && "${COMPARE}" "${COMPARE_WITH}" "${THIS_RUN}"
            # ignore failure exit from comparison
            exit 0

  create-concatenated-ui-monorepo-lock:
    description: "For UI monorepo concatenate all package.json files and yarn.lock to a single file"
    parameters:
      output-filename:
        type: string
    steps:
      - run:
          name: Concatenate package.json files and yarn.lock
          command: |
            find ui/ -type d \( -name node_modules \) -prune -false -o -name package.json -print0 | sort -z | xargs -r0 cat > << parameters.output-filename >>
            cat ui/yarn.lock >> << parameters.output-filename >>

  save-npm-deps-cache:
    description: "Prepare and save NPM dependencies cache"
    steps:
      - create-concatenated-ui-monorepo-lock:
          output-filename: ui/monorepo.lock

      - run:
          name: Calculate lock file digest
          command: |
            openssl dgst -r -sha256 ui/monorepo.lock >ui/monorepo.lock.sha256

      - save_cache:
          name: Save NPM dependencies cache
          key: v7-rox-npm-deps-{{ checksum "ui/monorepo.lock" }}
          paths:
            - ui/monorepo.lock.sha256
            - ui/deps
            - ~/.cache/Cypress # Cypress binary will be put there, see https://docs.cypress.io/guides/guides/continuous-integration.html#Example-circle-yml-v2-config-file-with-yarn
            - ui/node_modules # CircleCI doesn't support globbing here https://circleci.com/docs/2.0/caching/#basic-example-of-dependency-caching so all `node_nodules` need to be listed
            - ui/apps/platform/node_modules
            - ui/packages/ui-components/node_modules
            - ui/packages/tailwind-config/node_modules

  restore-npm-deps-cache:
    description: "Restore NPM dependencies cache"
    steps:
      - create-concatenated-ui-monorepo-lock:
          output-filename: ui/monorepo.lock

      - restore_cache:
          name: Restore NPM dependencies cache
          keys:
            - v7-rox-npm-deps-{{ checksum "ui/monorepo.lock" }}
            - v7-rox-npm-deps-

  fetch-ui-deps:
    description: "Fetch UI dependencies"
    steps:
      - run:
          name: Fetch dependencies
          command: |
            if [[ -d ui/node_modules && -f ui/monorepo.lock.sha256 && "$(cat ui/monorepo.lock.sha256)" == "$(openssl dgst -r -sha256 ui/monorepo.lock)" ]]; then
              exit 0  # cache is complete if the above condition matches
            fi
            make -C ui deps

jobs:
  ###
  # Jobs independent from build / compilation (like style checks, static code analysis, unit tests)
  ###

  slack-notify:
    executor: custom
    resource_class: small
    steps:
      - run:
          name: Send Slack notification with Workflow link
          command: |
            # Cannot use .circleci/is-nightly-tag.sh because this job doesn't have access to git
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              webhook_url="$NIGHTLY_WORKFLOW_NOTIFY_WEBHOOK"
            elif [[ "$CIRCLE_TAG" =~ ^[[:digit:]]+(\.[[:digit:]]+)*(-rc.[[:digit:]]+)?$ ]]; then
              webhook_url="$RELEASE_WORKFLOW_NOTIFY_WEBHOOK"
            else
              exit 0
            fi

            jq -n \
              --arg workflow_id "$CIRCLE_WORKFLOW_ID" \
              --arg tag "$CIRCLE_TAG" \
              '{"text": "CircleCI build for tag `\($tag)` started! Check the status of the build under the following URL: https://circleci.com/workflow-run/\($workflow_id)"}' \
            | curl -XPOST -d @- -H 'Content-Type: application/json' "$webhook_url"

  unit-tests:
    executor: custom
    resource_class: xlarge
    steps:
      - checkout

      - *restoreGoModCache
      - setup-go-build-env
      - *restoreGradle
      - restore-npm-deps-cache

      - *restoreGoBuildCache

      - run:
          name: Run Go unit tests
          command: make go-unit-tests

      - *saveGoBuildCache

      - run:
          name: Generate JUnit reports
          command: make generate-junit-reports
          when: always

      - store_test_results:
          path: junit-reports

      - store_artifacts:
          path: junit-reports
          destination: junit-reports

      - run:
          name: Run UI unit tests
          command: make ui-test

      - *storeUITestReports

      - run:
          name: Upload coverage information
          command: |
            retries=0
            success=0
            while (( retries < 5 && success == 0 )); do
              retries=$((retries + 1))
              success=1
              make upload-coverage || success=0
            done
          when: always

  style-checks:
    executor: custom
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache
      - setup-go-build-env
      - *restoreGradle
      - restore-npm-deps-cache

      - run:
          name: Run style checks
          command: make style

      - store_artifacts:
          path: qa-tests-backend/build/reports/codenarc
          destination: reports/codenarc

  check-generated-files-up-to-date:
    executor: custom
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache
      - setup-go-build-env
      - run:
          no_output_timeout: 30m
          name: Ensure that generated files are up to date. (If this fails, run `make go-generated-srcs` and commit the result.)
          command: |
            git ls-files --others --exclude-standard >/tmp/untracked
            make go-generated-srcs
            git diff --exit-code HEAD
            { git ls-files --others --exclude-standard ; cat /tmp/untracked ; } | sort | uniq -u >/tmp/untracked-new
            if [[ -s /tmp/untracked-new ]]; then
              echo 'Found new untracked files after running `make go-generated-srcs`. Did you forget to git add generated mocks?'
              cat /tmp/untracked-new
              exit 1
            fi

      - run:
          name: Ensure that all TODO references to fixed tickets are gone
          command: |
            .circleci/check-pr-fixes.sh

      - run:
          name: Ensure that there are no TODO references that the developer has marked as blocking a merge
          command: |
            # Matches comments of the form TODO(x), where x can be "DO NOT MERGE/don't-merge"/"dont-merge"/similar
            ./scripts/check-todos.sh 'do\s?n.*merge'

  integration-unit-tests:
    executor: custom
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache
      - setup-go-build-env

      - *restoreGoBuildCache

      - run:
          name: Run integration tests
          # Retry a few times if the tests fail since they can be flaky.
          command: |
            success=0
            max_tries=5
            for i in $(seq 1 "${max_tries}"); do
              if make integration-unit-tests; then
                success=1
                break
              fi
              echo "Retrying (failed ${i} times)"
            done
            if [[ "${success}" == 0 ]]; then
              echo "Failed after ${max_tries} tries"
              exit 1
            fi

      - *saveGoBuildCache

  designated-image-tests:

    ###
    # This job is intended to run qa-backend-tests against images that are released
    # without a full rox CI run. e.g. the DSOP images that are repackaged with a
    # different base image.
    ##

    # To trigger a run you might do something like:
    #
    #   curl -X POST --header 'Content-Type: application/json' \
    #        --header "Circle-Token: <your CI token>" https://circleci.com/api/v2/project/github/stackrox/rox/pipeline -d '{
    #     "branch": "3.0.50.x",
    #     "parameters": {
    #         "run_designated_image_test": true,
    #         "main_image_tag": "3.0.50.1",
    #         "main_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/main-rhel",
    #         "roxctl_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/main-rhel",
    #         "scanner_db_image": "registry1.dsop.io/ironbank/stackrox/stackrox/scanner-db-rhel:2.5.0",
    #         "scanner_image": "registry1.dsop.io/ironbank/stackrox/stackrox/scanner-rhel:2.5.0",
    #         "collector_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/collector-rhel",
    #         "registry_username": "<your DSOP registry1 username>",
    #         "registry_password": "<token from registry1>",
    #         "rox_license_key": "<a valid ROX license>"
    #     }
    # }'

    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "true"
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: none
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - AUTH0_SUPPORT: false
    steps:
      - checkout

      - run:
          name: Parameter check
          command: |
            some_missing=0

            function check_and_set() {
                local name="$1"
                local value="$2"
                if [[ -z "${value}" ]]; then
                    echo "Parameter ${name} is missing"
                    some_missing=1
                else
                    cci-export "$(echo "$name" | tr a-z A-Z)" "${value}"
                fi
            }

            check_and_set "main_image_tag" "<< pipeline.parameters.main_image_tag >>"
            check_and_set "main_image_repo" "<< pipeline.parameters.main_image_repo >>"
            check_and_set "roxctl_image_repo" "<< pipeline.parameters.roxctl_image_repo >>"
            check_and_set "scanner_db_image" "<< pipeline.parameters.scanner_db_image >>"
            check_and_set "scanner_image" "<< pipeline.parameters.scanner_image >>"
            check_and_set "collector_image_repo" "<< pipeline.parameters.collector_image_repo >>"
            check_and_set "registry_username" "<< pipeline.parameters.registry_username >>"
            check_and_set "registry_password" "<< pipeline.parameters.registry_password >>"
            check_and_set "rox_license_key" "<< pipeline.parameters.rox_license_key >>"
            
            if (( some_missing )); then
                echo "Some required parameters are missing. Exiting."
                exit 1
            fi
          
      - provision-gke-cluster:
          cluster-id: "designated-image-tests"

      - setup_remote_docker

      - *setupGoogleAppCreds

      - *setupDefaultTLSCerts

      - run:
          name: Get roxctl from released artifacts
          command: |
            gsutil cp "gs://sr-roxc/${MAIN_IMAGE_TAG}/bin/linux/roxctl" "$GOPATH/bin/roxctl"
            chmod +x "$GOPATH/bin/roxctl"
            roxctl version

      - deploy-stackrox:
          validate-sensor-bundle: false
          require-cluster-admin: true
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - run:
          name: Create a SR dockerhub secret for test image access
          command: |
            kubectl -n stackrox create secret docker-registry stackrox-dockerhub \
              --docker-server="docker.io" \
              --docker-username="${DOCKER_IO_PULL_USERNAME}" \
              --docker-password="${DOCKER_IO_PULL_PASSWORD}" \
              --docker-email="test@stackrox.com"

      - deploy-default-psp
      - deploy-webhook-server
      - deploy-authz-plugin

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform
          command: |
            export REGISTRY_USERNAME="${DOCKER_IO_PULL_USERNAME}"
            export REGISTRY_PASSWORD="${DOCKER_IO_PULL_PASSWORD}"
            export CLUSTER=K8S
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            make -C qa-tests-backend bat-test

      - *storeQATestResults
      - *storeQASpockReports

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs
      - *storeQALogs

      # - teardown-gke
  
  ###
  # Build jobs
  ###

  pre-build-ui:
    executor: custom
    resource_class: large
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - restore-npm-deps-cache
      - fetch-ui-deps

      - run:
          name: Prepare UI tree state file
          command: |
            git log -n 1 --pretty=format:%H -- ui/ >/tmp/ui-tree-state
            # Add the node version to the UI tree state, since a different node version
            # should bust the cache.
            node -v >>/tmp/ui-tree-state

      - restore_cache:
          name: Restoring UI build cache
          key: ui-build-cache-v1-{{ checksum "/tmp/ui-tree-state" }}

      - run:
          name: Build UI
          command: |
            if [[ -d ui/build ]]; then
              exit 0  # build cache is always complete
            fi
            make -C ui build

      - save_cache:
          name: Saving UI build cache
          key: ui-build-cache-v1-{{ checksum "/tmp/ui-tree-state" }}
          paths:
            - ui/build

      - save-npm-deps-cache

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - ui/build # Copied directly into the image downstream.
            - ui/deps
            - ui/node_modules # node_modules dirs here and below are used for OSS license auditing downstream
            - ui/apps/*/node_modules
            - ui/packages/*/node_modules

      - cleanup-clusters-on-fail

  pre-build-cli:
    executor: custom
    resource_class: large
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - *restoreGoModCache
      - setup-go-build-env
      - *restoreGoBuildCache
      - run:
          name: Build the CLI
          command: make cli

      - run:
          name: Validate expected Go version
          command: |
            roxctl_go_version="$(roxctl version --json | jq '.GoVersion' -r)"
            expected_go_version="$(cat EXPECTED_GO_VERSION)"
            if [[ "${roxctl_go_version}" != "${expected_go_version}" ]]; then
              echo "Got unexpected go version ${roxctl_go_version} (wanted ${expected_go_version})"
              exit 1
            fi

            # Ensure that the Go version is up-to-date in go.mod as well.
            # Note that the patch version is not specified in go.mod.
            [[ "${expected_go_version}" =~ ^go(1\.[0-9]{2})(\.[0-9]+)?$ ]]
            go_version="${BASH_REMATCH[1]}"
            go mod edit -go "${go_version}"
            git diff --exit-code -- go.mod
      - *saveGoModCache

      - *saveGoBuildCache

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - bin/linux/roxctl
            - bin/darwin/roxctl
            - bin/windows/roxctl.exe

      - cleanup-clusters-on-fail

  pre-build-go-binaries:
    machine: true
    working_directory: ~/go/src/github.com/stackrox/rox
    environment:
      - GOPATH: /home/circleci/go
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    resource_class: large
    steps:
      - checkout
      - run:
          name: Create cci-export utility and setup env
          command: |
            sudo mv /bin/bash /bin/real-bash
            sudo cp .circleci/bash-wrapper /bin/bash
            sudo cp .circleci/bash.env /etc/bash.env
      - run:
          name: Install Go
          command: |
            # We need to install go here to pull go modules, even though the build itself is dockerized.
            sudo rm -rf /usr/local/go
            go_version="$(cat EXPECTED_GO_VERSION)" # This is already prefixed with `go`.
            wget -c "https://storage.googleapis.com/golang/${go_version}.linux-amd64.tar.gz"
            sudo tar -C /usr/local -xvzf "${go_version}.linux-amd64.tar.gz"
            cci-export PATH $PATH:$GOPATH/bin
      - *restoreGoModBinaryCache
      - setup-go-build-env
      - *restoreGoBuildBinaryCache
      - setup-dep-env:
          docker-login: true

      - run:
          name: Build the main Go binaries
          command: make main-build

      - *saveGoBuildBinaryCache
      - *saveGoModBinaryCache

      - run:
          name: Generate the swagger docs
          command: make swagger-docs

      - persist_to_workspace:
          root: ~/go/src/github.com/stackrox/rox
          paths:
            - bin/linux/central
            - bin/linux/migrator
            - bin/linux/kubernetes
            - bin/linux/admission-control
            - bin/linux/upgrader
            - bin/linux/collection
            - image/docs # This will go into the image as generated docs.
            - image/keys
            - deps # Used to speed up k8s-tests
            - generated # Used to speed up k8s-tests

      - cleanup-clusters-on-fail

  build:
    executor: custom
    resource_class: large
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - setup_remote_docker
      - setup-dep-env:
          docker-login: true

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *refreshAlpineBaseImage

      - install-ossls

      - *restoreGoModCache
      - setup-go-build-env

      - run:
          name: Generate OSS notice
          command: |
            make ossls-notice

      - run:
          name: Log in to Docker Hub to pull the docs image
          command: |
            docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD" docker.io

      - run:
          name: Build main image
          command: make docker-build-main-image      
      
      - run:
          name: Build roxctl image
          command: make docker-build-roxctl-image

      - run:
          name: Push new Docker image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            docker push "docker.io/stackrox/main:$(make --quiet tag)" | cat
            docker push "docker.io/stackrox/roxctl:$(make --quiet tag)" | cat

      - run:
          name: Publish new versions of NPM packages if any
          command: |
            if [[ "${CIRCLE_BRANCH}" == "master" ]]; then
              echo "//npm.pkg.github.com/:_authToken=\"$GITHUB_PACKAGES_PUBLISH_TOKEN\"" > ~/.npmrc
              make ui-publish-packages
            else
              echo "Not on master, skipping publishing NPM packages."
            fi

      - *saveGoModCache

      - store_artifacts:
          path: bin/linux/roxctl
          destination: roxctl/roxctl-linux

      - store_artifacts:
          path: bin/darwin/roxctl
          destination: roxctl/roxctl-darwin

      - run:
          name: Comment on PR
          command: |
            wget --quiet https://github.com/joshdk/hub-comment/releases/download/0.1.0-rc6/hub-comment_linux_amd64
            sudo install hub-comment_linux_amd64 /usr/bin/hub-comment

            export TAG=$(make --quiet tag)
            hub-comment -template-file .circleci/comment-template.tpl

      - cleanup-clusters-on-fail

  build-rhel:
    executor: custom
    resource_class: large
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - setup_remote_docker

      - install-ossls

      - *restoreGoModCache
      - setup-go-build-env

      - restore-npm-deps-cache
      - fetch-ui-deps

      - run:
          name: Generate OSS notice
          command: |
            make ossls-notice

      - run:
          name: Log in to Docker Hub to pull the docs image
          command: |
            docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD" docker.io

      - run:
          name: Build main image for RHEL which includes building the binaries because they need to be built specifically for RHEL
          command: |
            if .circleci/is-nightly-tag.sh || .circleci/pr_has_label.sh ci-race-tests; then
              # build race images and then overwrite
              RACE=true make main-image-rhel
            else
              make main-image-rhel
            fi

      - run:
          name: Push new image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG="$(make --quiet tag)"
            docker push "docker.io/stackrox/main-rhel:${TAG}" | cat

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - image/rhel/bundle.tar.gz

      - cleanup-clusters-on-fail

  build-deployer:
    executor: custom
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Build deployer image for GCP Marketplace
          command: make docker-build-deployer-image

      - run:
          name: Push new image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG="$(make --quiet tag)"
            docker push "docker.io/stackrox/deployer:${TAG}" | cat

  ###
  # Jobs that build and provision test resources (like cluster provisioning)
  # Recommended naming format: provision-{test_job_name}
  # Note: cluster provisioning job names used as resource label values, they must contain only letters and '-'
  ###

  provision-gke-api-nongroovy-tests:
    executor: custom
    steps:
      - checkout
      - check-backend-changes
      - check-label-to-skip-tests:
          label: ci-no-api-tests
      - provision-gke-cluster:
          cluster-id: api-nongroovy-tests
          num-nodes: 2

  provision-gke-ui-e2e-tests:
    executor: custom
    steps:
      - checkout
      - check-label-to-skip-tests:
          label: ci-no-ui-tests
      - provision-gke-cluster:
          cluster-id: ui-e2e-tests
          num-nodes: 2

  provision-gke-api-e2e-tests:
    executor: custom
    environment:
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "true"
    steps:
      - checkout
      - check-backend-changes
      - check-label-to-skip-tests:
          label: ci-no-qa-tests

      - provision-gke-cluster:
          cluster-id: api-e2e-tests

  provision-openshift-rhel-api-e2e-tests:
    machine: true
    environment:
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "true"
    steps:
      - checkout
      - check-backend-changes
      - check-master-or-tag-or-label:
          label: ci-rhel-tests,ci-race-tests
          run-on-master: false
          run-on-tags: true

      - provision-openshift-cluster:
          crio: false
          suffix: -rhel
          version: 0.1.1

  provision-gke-rhel-api-e2e-tests:
    executor: custom
    environment:
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "true"
    steps:
      - checkout
      - check-backend-changes
      - check-master-or-tag-or-label:
          label: ci-rhel-tests,ci-race-tests
          run-on-master: false
          run-on-tags: true

      - provision-gke-cluster:
          cluster-id: rhel-api-e2e-tests

  provision-gke-kernel-api-e2e-tests:
    executor: custom
    steps:
      - checkout
      - check-backend-changes

      - check-master-or-tag-or-label:
          label: ci-kernel-module-tests
          run-on-master: true
          run-on-tags: true

      - provision-gke-cluster:
          cluster-id: kernel-api-e2e-tests

  provision-gke-api-upgrade-tests:
    executor: custom
    environment:
      # Use 1.15 to facilitate the upgrade install of 2.4.16.4
      - CLUSTER_VERSION: 1.15.
    steps:
      - checkout
      - check-backend-changes
      - check-label-to-skip-tests:
          label: ci-no-upgrade-tests

      - provision-gke-cluster:
          cluster-id: upgrade-test

  provision-test-helm-charts-repo:
    executor: custom
    steps:
      - checkout
      - provision-gke-cluster:
          cluster-id: test-helm-charts
          num-nodes: 2

  build-scale-monitoring-and-mock-server:
    executor: custom
    resource_class: large
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - setup_remote_docker

      - *refreshAlpineBaseImage

      - *restoreGoModCache
      - setup-go-build-env
      - *restoreGoBuildCache

      - run:
          name: Build images
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            make scale-image mock-grpc-server-image monitoring-image

      - *saveGoBuildCache

      - run:
          name: Push new Docker image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG=$(make --quiet tag)

            for img in scale grpc-server; do
              docker push "docker.io/stackrox/${img}:${TAG}" | cat
            done

      - cleanup-clusters-on-fail

  provision-gke-api-scale-tests:
    executor: custom
    steps:
      - checkout
      - check-backend-changes
      - check-label-exists-or-skip:
          label: ci-scale-tests
          run-on-master: false
      - provision-gke-cluster:
          cluster-id: scale-test
          machine-type: e2-standard-8

  provision-openshift-api-e2e-tests:
    machine: true
    steps:
      - provision-openshift-cluster:
          crio: false
          gate-label: ci-openshift-tests
          version: 0.1.1

  provision-openshift-crio-api-e2e-tests:
    machine: true
    steps:
      - provision-openshift-cluster:
          crio: true
          gate-label: ci-openshift-crio-tests
          suffix: -crio
          version: 0.1.1

  provision-eks-api-e2e-tests:
    executor: custom
    environment:
      - EKS_AUTOMATION_VERSION: 0.1.5
    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-eks-tests
          run-on-master: false

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *loginToGCR
      - run:
          name: Create EKS cluster
          command: |
            mkdir $PWD/eks
            docker run --name eks-automation -t \
              -v $PWD/eks:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION create "${CIRCLE_WORKFLOW_ID:0:7}"
            docker cp eks-automation:/data $PWD/eks

      - run:
          name: Validate Kubeconfig
          command: |
            ls -lh $PWD/eks/data
            export KUBECONFIG=$PWD/eks/data/eks-kube.yaml
            sudo chown "$USER" $PWD/eks/data/eks-kube.yaml
            kubectl get nodes

      - store_artifacts:
          path: eks
          destination: eks

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - eks

      - run:
          name: Destroy EKS cluster
          command: |
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

  provision-kops-api-e2e-tests:
    machine: true
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-kops-tests
          run-on-master: false

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - run:
          name: Install kubectl
          working_directory: /tmp
          command: |
            wget https://storage.googleapis.com/kubernetes-release/release/v1.13.3/bin/linux/amd64/kubectl
            sudo install kubectl /usr/bin

      - store_artifacts:
          path: kops
          destination: kops

      - *loginToGCR
      - run:
          name: Create Kops cluster
          command: |
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              -e TF_VAR_creation_source=CI \
              gcr.io/stackrox-infra/automation-flavors/kops:$KOPS_AUTOMATION_VERSION create "${CIRCLE_WORKFLOW_ID:0:7}"
            sudo chown "$USER" kops/id_rsa*

      - run:
          name: Add /etc/hosts entry
          command: echo "$(cat $PWD/kops/master_ip) api.$(cat $PWD/kops/cluster_name)" | sudo tee -a /etc/hosts

      - run:
          name: Configure Kubeconfig
          command: |
            ls -lh $PWD/kops
            export KUBECONFIG=$PWD/kops/kube.yaml
            sudo chown "$USER" $PWD/kops/kube.yaml
            kubectl get nodes

      - store_artifacts:
          path: kops
          destination: kops

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - kops

      - run:
          name: Destroy Kops cluster
          command: |
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/kops:$KOPS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

  ###
  # Test jobs against built artifacts
  # Recommended naming format: {cluster_type}-[{flavor}-]{test_target_type}-{any_test_name}-tests
  #   e.g. 'gke-ui-e2e-tests' or 'openshift-crio-api-e2e-tests'
  ###

  gke-api-nongroovy-tests:
    executor: custom
    environment:
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - LOAD_BALANCER: lb
      - ROX_PLAINTEXT_ENDPOINTS: "8080,grpc@8081"
      - ROXDEPLOY_CONFIG_FILE_MAP: "scripts/ci/endpoints/endpoints.yaml"
      - ROX_ADMISSION_CONTROL_SERVICE: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_FLOWS_SEARCH_FILTER_UI: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true      

    steps:
      - checkout
      - check-backend-changes

      - check-label-to-skip-tests:
          label: ci-no-api-tests

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *restoreGoModCache
      - setup-go-build-env
      - run:
          name: Retrieving missing dependencies
          command: make download-deps

      - *setupRoxctl
      - setup_remote_docker
      - setup-gcp
      - setup-dep-env

      - attach-gke-cluster:
          cluster-id: api-nongroovy-tests
      - remove-existing-stackrox-resources

      - run:
          name: Export Trusted CA file
          command: cci-export TRUSTED_CA_FILE "$PWD/tests/bad-ca/untrusted-root-badssl-com.pem"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          require-cluster-admin: true
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - run:
          name: Set up client CA auth provider
          command: |
            roxctl -e "$API_ENDPOINT" -p "$ROX_PASSWORD" \
              central userpki create test-userpki -r None -c "$CLIENT_CA_PATH"

      - run:
          name: Test roxctl command for cert gen
          command: |
            mkdir gencerts
            roxctl -e "$API_ENDPOINT" -p "$ROX_PASSWORD" \
              sensor generate-certs remote --output-dir gencerts
            [[ -f gencerts/cluster-remote-tls.yaml ]]
            # Use the certs in future steps that will use client auth.
            # This will ensure that the certs are valid.
            sensor_tls_cert="$(kubectl create --dry-run=client -o json -f gencerts/cluster-remote-tls.yaml | jq 'select(.metadata.name=="sensor-tls")')"
            for file in ca.pem sensor-cert.pem sensor-key.pem; do
              echo "${sensor_tls_cert}" | jq --arg filename "${file}" '.data[$filename]' -r | base64 --decode > "gencerts/${file}"
            done

      - run:
          name: Set up environment for endpoints test
          command: |
            # Use the files from the ones generated by the generate-certs command above.
            cci-export "SERVICE_CA_FILE" "${PWD}/gencerts/ca.pem"
            cci-export "SERVICE_CERT_FILE" "${PWD}/gencerts/sensor-cert.pem"
            cci-export "SERVICE_KEY_FILE" "${PWD}/gencerts/sensor-key.pem"

            central_pod="$(kubectl -n stackrox get po -lapp=central -oname | head -n 1)"
            for target_port in 8080 8081 8082 8443 8444 8445 8446 8447 8448; do
              nohup kubectl -n stackrox port-forward "${central_pod}" "$((target_port + 10000)):${target_port}" </dev/null &>/dev/null &
            done
            sleep 1

      - run:
          name: Test roxctl semantics of --token-file parameter
          command: |
            API_ENDPOINT="$API_ENDPOINT" ROX_PASSWORD="$ROX_PASSWORD" ./tests/roxctl/token-file.sh

      - run:
          name: Test slim collector mode with roxctl
          command: |
            API_ENDPOINT="$API_ENDPOINT" ROX_PASSWORD="$ROX_PASSWORD" ./tests/roxctl/slim-collector.sh

      - run:
          name: Test istio-support flag in roxctl
          command: |
            API_ENDPOINT="$API_ENDPOINT" ROX_PASSWORD="$ROX_PASSWORD" ./tests/roxctl/istio-support.sh

      - run:
          name: Test Helm chart generation in roxctl
          command: |
             MAIN_TAG=$(make --quiet tag) ./tests/roxctl/helm-chart-generation.sh

      - run:
          name: API tests
          command: |
            CA="${SERVICE_CA_FILE}" ./tests/yamls/roxctl_verification.sh
            make -C tests

      - run:
          name: Deploy NGINX proxies
          command: |
            export PROXY_CERTS_DIR="$(mktemp -d)"
            scripts/ci/proxy/deploy.sh
            cci-export PROXY_CERTS_DIR "$PROXY_CERTS_DIR"

      - run:
          name: Port-forward to NGINX proxies
          command: |
            nohup kubectl -n proxies port-forward svc/nginx-proxy-plain-http 10080:80 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-multiplexed 10443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-multiplexed-tls-be 11443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http1 12443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http1-plain 13443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http2 14443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http2-plain 15443:443 </dev/null &>/dev/null &
            sleep 1

      - run:
          name: Add an /etc/hosts entry
          command: |
            sudo bash -c 'echo "127.0.0.1 central-proxy.stackrox.local" >>/etc/hosts'

      - run:
          name: Test HTTP access to plain HTTP proxy
          command: |
            # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
            version="$(curl --retry 5 --retry-connrefused -4 --retry-delay 1 --retry-max-time 10 -f 'http://central-proxy.stackrox.local:10080/v1/metadata' | jq -r '.version')"
            echo "Got version ${version} from server"
            [[ "$version" == "$(make --quiet tag)" ]]

      - run:
          name: Test HTTPS access to multiplexed TLS proxy
          command: |
            # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
            version="$(
              curl --cacert "${PROXY_CERTS_DIR}/ca.crt" \
                --retry 5 --retry-connrefused -4 --retry-delay 1 --retry-max-time 10 \
                -f \
                'https://central-proxy.stackrox.local:10443/v1/metadata' | jq -r '.version')"
            echo "Got version ${version} from server"
            [[ "$version" == "$(make --quiet tag)" ]]

      - run:
          name: Test roxctl access to proxies
          command: |
            proxies=(
              "Plaintext proxy:10080:plaintext"
              "Multiplexed TLS proxy with plain backends:10443"
              "Multiplexed TLS proxy with TLS backends:11443"
              "Multiplexed TLS proxy with plain backends (direct gRPC):10443:direct"
              "Multiplexed TLS proxy with TLS backends (direct gRPC):11443:direct"
              "HTTP/1 proxy with TLS backends:12443"
              "HTTP/1 proxy with plain backends:13443"
              "HTTP/2 proxy with TLS backends:14443"
              "HTTP/2 proxy with plain backends:15443"
            )

            failures=()
            for p in "${proxies[@]}"; do
              name="$(echo "$p" | cut -d: -f1)"
              port="$(echo "$p" | cut -d: -f2)"
              opt="$(echo "$p" | cut -d: -f3)"
              mkdir -p "/tmp/proxy-test-${port}-${opt}" && cd "/tmp/proxy-test-${port}-${opt}"

              extra_args=()
              scheme="https"
              plaintext="false"
              plaintext_neg="true"
              direct=0
              case "$opt" in
                plaintext)
                  extra_args=(--insecure)
                  plaintext="true"
                  plaintext_neg="false"
                  scheme="http"
                  ;;
                direct)
                  extra_args=(--direct-grpc)
                  direct=1
                  ;;
              esac

              echo "Testing roxctl access through ${name}..."
              endpoint="central-proxy.stackrox.local:${port}"
              for endpoint_tgt in "${scheme}://${endpoint}" "${scheme}://${endpoint}/" "$endpoint"; do
                roxctl "${extra_args[@]}" --plaintext="$plaintext" -e "${endpoint_tgt}" -p "$ROX_PASSWORD" central debug log >/dev/null || \
                  failures+=("$p")

                if (( direct )); then
                  roxctl "${extra_args[@]}" --plaintext="$plaintext" --force-http1 -e "${endpoint_tgt}" -p "$ROX_PASSWORD" central debug log &>/dev/null && \
                    failures+=("${p},force-http1")
                else
                  roxctl "${extra_args[@]}" --plaintext="$plaintext" --force-http1 -e "${endpoint_tgt}" -p "$ROX_PASSWORD" central debug log >/dev/null || \
                    failures+=("${p},force-http1")
                fi

                if [[ "$endpoint_tgt" = *://* ]]; then
                  # Auto-sense plaintext or TLS when specifying a scheme
                  roxctl "${extra_args[@]}" -e "${endpoint_tgt}" -p "$ROX_PASSWORD" central debug log >/dev/null || \
                    failures+=("${p},tls-autosense")

                  # Incompatible plaintext configuration should fail
                  roxctl "${extra_args[@]}" --plaintext="$plaintext_neg" -e "${endpoint_tgt}" -p "$ROX_PASSWORD" central debug log &>/dev/null && \
                    failures+=("${p},incompatible-tls")
                fi

              done
              roxctl "${extra_args[@]}" --plaintext="$plaintext" -e "central-proxy.stackrox.local:${port}" -p "$ROX_PASSWORD" sensor generate k8s --name remote --continue-if-exists || \
                failures+=("${p},sensor-generate")
              echo "Done."
              rm -rf "/tmp/proxy-test-${port}"
            done

            echo "Total: ${#failures[@]} failures."
            if (( ${#failures[@]} > 0 )); then
              printf " - %s\n" "${failures[@]}"
              exit 1
            fi

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs

      - run:
          name: Deploy and restore DB to version 41.4
          command: |
            # Download 41.4 backup
            gsutil cp gs://stackrox-ci-upgrade-test-dbs/41.4-upgrade-db.zip .

            # Restore 41.4 backup
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m 41.4-upgrade-db.zip

      - *waitForAPI

      - run:
          name: Run tests relating to external backups
          command: |
            make -C tests external-backup-tests

      - run:
          name: Destructive API tests
          command: |
            make -C tests destructive-tests

      - run:
          name: Collect final stackrox logs
          command: |
            ./scripts/ci/collect-service-logs.sh stackrox /tmp/final-stackrox-logs
          when: always

      - store_artifacts:
          path: /tmp/final-stackrox-logs
          destination: final-stackrox-logs

      - *storeK8sLogs
      - teardown-gke

  gke-ui-e2e-tests:
    executor: custom
    environment:
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - LOAD_BALANCER: lb
      - ROX_EVENT_TIMELINE_UI: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_AWS_SECURITY_HUB_INTEGRATION: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - checkout
      - check-label-to-skip-tests:
          label: ci-no-ui-tests

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup_remote_docker
      - setup-gcp
      - setup-dep-env

      - attach-gke-cluster:
          cluster-id: ui-e2e-tests
      - remove-existing-stackrox-resources

      - *setupGoogleAppCreds
      - *setupLicense
      - *setupDefaultTLSCerts

      - deploy-stackrox:
          require-cluster-admin: true
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - restore-npm-deps-cache
      - *determineWhetherToRunUIDevServer
      - *runUIDevServer
      - *waitForUIDevServer
      - *runUIE2E

      - *collectImbuedUILogs
      - *storeImbuedUILogs

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs

      - teardown-gke

      - *storeUITestReports
      - *storeUITestArtifacts

  gke-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - run-qa-tests:
          cluster-id: api-e2e-tests
          sensor-deploy-flavor: helm
          determine-whether-to-run:
            - check-label-to-skip-tests:
                label: ci-no-qa-tests
          use-websocket: true

  openshift-rhel-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ROX_SENSOR_DETECTION: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - checkout
      - check-backend-changes
      - check-master-or-tag-or-label:
          label: ci-rhel-tests,ci-race-tests
          run-on-master: false
          run-on-tags: true
      - run-openshift-tests:
          suffix: -rhel
          version: 0.1.1
          use-main-rhel: true
          sensor-deploy-flavor: helm

  gke-rhel-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - run-qa-tests:
          cluster-id: rhel-api-e2e-tests
          # sensor-deploy-flavor: helm TODO(ROX-4606)
          determine-whether-to-run:
            - check-master-or-tag-or-label:
                label: ci-rhel-tests,ci-race-tests
                run-on-master: false
                run-on-tags: true
          use-main-rhel: true
          use-websocket: true

  gke-kernel-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_ROCKSDB: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - OUTPUT_FORMAT: helm
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - run-qa-tests:
          cluster-id: kernel-api-e2e-tests
          determine-whether-to-run:
            - check-master-or-tag-or-label:
                label: ci-kernel-module-tests
                run-on-master: true
                run-on-tags: true

  gke-api-upgrade-tests:
    executor: custom
    environment:
      - STORAGE: pvc
      - LOAD_BALANCER: lb

    steps:
      - checkout
      - check-backend-changes
      - check-label-to-skip-tests:
          label: ci-no-upgrade-tests

      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - setup-dep-env
      - attach-gke-cluster:
          cluster-id: upgrade-test
      - remove-existing-stackrox-resources

      - *setupGoogleAppCreds

      - run:
          name: Setup CI License
          command: |
            ROX_LICENSE_KEY="$(licenses/ci.sh)"
            cci-export ROX_LICENSE_KEY "$ROX_LICENSE_KEY"

      - run:
          name: Deploy Central version right after 41.4
          command: |
            git checkout 309f7d8edc03e9767ddabefad670224e4bd1746f
            # We explicitly need to set CIRCLE_TAG to empty, as it will otherwise dictate
            # the output of `make tag`.
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag CIRCLE_TAG=)"
            ./deploy/k8s/central.sh

            # Remove the scanner HPA for these tests to avoid resource throttling
            kubectl -n stackrox delete hpa/scanner

            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - *waitForAPI

      - run:
          name: Deploy and restore DB to version 41.4
          command: |
            # Download 41.4 backup
            gsutil cp gs://stackrox-ci-upgrade-test-dbs/41.4-upgrade-db.zip .

            # Restore 41.4 backup to 41.4 based commit
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m 41.4-upgrade-db.zip

      - *waitForAPI

      - run:
          name: Checkout current commit again
          command: git reset --hard "${CIRCLE_SHA1}"

      - run:
         name: Do the upgrade
         command: |
          central_image="stackrox/main:$(make --quiet tag)"
          kubectl -n stackrox set image deploy/central central=${central_image}

          scanner_image="stackrox/scanner:$(cat SCANNER_VERSION)"
          kubectl -n stackrox set image deploy/scanner scanner=${scanner_image}

          scanner_db_image="stackrox/scanner-db:$(cat SCANNER_VERSION)"
          kubectl -n stackrox set image deploy/scanner-db db=${scanner_db_image}

      - *waitForAPI

      - validate-upgrade:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Run upgrade tests on migrated version"

      - *restoreGradle

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: upgrade-test

      - *updateTestRail

      - run:
          name: Fetch bundle for the "remote" cluster in the DB
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" sensor get-bundle remote
            [[ -d sensor-remote ]]

            ## Edit the main image and collector image in the YAMLs to be the latest ones.
            ## Note that we do NOT want to patch the cluster object directly since we want to ensure
            ## that an unpatched cluster object works.
            main_image="${MAIN_IMAGE_REPO:-docker.io/stackrox/main}:$(make --quiet tag)"
            sed -i 's@docker.io/stackrox/main.*@'"${main_image}"'@g' ./sensor-remote/sensor.yaml
            collector_image="${COLLECTOR_IMAGE_REPO:-docker.io/stackrox/collector}:$(cat COLLECTOR_VERSION)-latest"
            sed -i 's@docker.io/stackrox/collector.*@'"${collector_image}"'@g' ./sensor-remote/sensor.yaml

      - run:
          name: Install the sensor
          command: ./sensor-remote/sensor.sh

      - run:
          name: "Patch down the sensor resources"
          command: |
            kubectl -n stackrox patch deploy/sensor --patch '{"spec":{"template":{"spec":{"containers":[{"name":"sensor","resources":{"limits":{"cpu":"500m","memory":"500Mi"},"requests":{"cpu":"500m","memory":"500Mi"}}}]}}}}'

      - *waitForSensorK8s

      - run:
          name: Delete the sensor
          command: |
            ./sensor-remote/delete-sensor.sh
            rm -rf sensor-remote

      - run:
          name: Install the metrics server, and inactivate it, to reproduce ROX-4429
          command: |
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml

            echo "Waiting for metrics.k8s.io to be in kubectl API resources..."
            success=0
            for i in $(seq 1 10); do
              if kubectl api-resources | grep metrics.k8s.io; then
                success=1
                break
              fi
              sleep 5
            done
            [[ "${success}" -eq 1 ]]

            ## Patch the metrics server to be unreachable
            kubectl -n kube-system patch svc/metrics-server --type json -p '[{"op": "replace", "path": "/spec/selector", "value": {"k8s-app": "non-existent"}}]'

            echo "Waiting for metrics.k8s.io to NOT be in kubectl API resources..."
            success=0
            for i in $(seq 1 10); do
              kubectl api-resources >stdout.out 2>stderr.out || true
              if grep 'metrics.k8s.io.*the server is currently unable to handle the request' stderr.out; then
                success=1
                break
              fi
              echo "metrics.k8s.io still in API resources. Will try again..."
              cat stdout.out
              cat stderr.out
              sleep 5
            done
            [[ "${success}" -eq 1 ]]
            rm -f stdout.out stderr.out

      - run:
          name: Create a "sensor-remote-new" cluster
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" sensor generate k8s \
              --main-image-repository "${MAIN_IMAGE_REPO:-docker.io/stackrox/main}" \
              --collector-image-repository "${COLLECTOR_IMAGE_REPO:-docker.io/stackrox/collector}" \
              --name remote-new \
              --create-admission-controller

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 3b2cbf78-d35a-4c2c-b67b-e37f805c14da
          desc: for the first time, to test rollback

      - run:
          name: Roll back sensor upgrade
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5

            ROX_UPGRADE_PROCESS_ID=3b2cbf78-d35a-4c2c-b67b-e37f805c14da \
                ROX_CENTRAL_ENDPOINT="${API_ENDPOINT}" \
                ROX_MTLS_CA_FILE="$(pwd)/sensor-remote-new/ca.pem" \
                ROX_MTLS_CERT_FILE="$(pwd)/sensor-remote-new/sensor-cert.pem" \
                ROX_MTLS_KEY_FILE="$(pwd)/sensor-remote-new/sensor-key.pem" \
                KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              ./bin/linux/upgrader -workflow roll-back -kube-config kubectl

            kill "$proxy_pid"

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 9b8f2cbb-72c6-4e00-b375-5b50ce7f988b
          desc: from scratch

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 9b8f2cbb-72c6-4e00-b375-5b50ce7f988b
          desc: again, but with the same upgrade process ID

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 789c9262-5dd3-4d58-a824-c2a099892bd6
          desc: yet again, but with a new upgrade process ID

      - run:
          name: Patch admission webhook
          command: |
            kubectl -n stackrox patch validatingwebhookconfiguration stackrox --type 'json'  -p '[{"op":"replace","path":"/webhooks/0/timeoutSeconds","value":29}]'
            if [[ "$(kubectl -n stackrox get validatingwebhookconfiguration/stackrox -o json  | jq '.webhooks | .[0] | .timeoutSeconds')" -ne 29 ]]; then
              echo "Webhook not patched";
              kubectl -n stackrox get validatingwebhookconfiguration/stackrox -o yaml
              exit 1;
            fi

      - run:
          name: Patch resources
          command: |
            kubectl -n stackrox set resources deploy/sensor -c sensor --requests 'cpu=1.1,memory=1.1Gi'

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 060a9fa6-0ed6-49ac-b70c-9ca692614707
          desc: after manually patching webhook

      - run:
          name: Verify the webhook was patched back by the upgrader
          command: |
            if [[ "$(kubectl -n stackrox get validatingwebhookconfiguration/stackrox -o json  | jq '.webhooks | .[0] | .timeoutSeconds')" -ne 30 ]]; then
              echo "Webhook not patched";
              kubectl -n stackrox get validatingwebhookconfiguration/stackrox -o yaml
              exit 1;
            fi

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 789c9262-5dd3-4d58-a824-c2a099892bd7
          desc: with yet another new ID

      - run:
          name: Verify resources were patched back by the upgrader
          command: |
            resources="$(kubectl -n stackrox get deploy/sensor -o 'jsonpath=cpu={.spec.template.spec.containers[?(@.name=="sensor")].resources.requests.cpu},memory={.spec.template.spec.containers[?(@.name=="sensor")].resources.requests.memory}')"
            if [[ "$resources" != 'cpu=1,memory=1Gi' ]]; then
              echo "Resources ($resources) not patched back!"
              kubectl -n stackrox get deploy/sensor -o yaml
              exit 1
            fi

      - run:
          name: Patch resources and add preserve resources annotation. Also, check toleration preservation
          command: |
            kubectl -n stackrox annotate deploy/sensor "auto-upgrade.stackrox.io/preserve-resources=true"
            kubectl -n stackrox set resources deploy/sensor -c sensor --requests 'cpu=1.1,memory=1.1Gi'
            kubectl -n stackrox patch deploy/sensor -p '{"spec":{"template":{"spec":{"tolerations":[{"effect":"NoSchedule","key":"thekey","operator":"Equal","value":"thevalue"}]}}}}'

      - deploy-sensor-via-upgrader-for-upgrade-tests:
          upgrade-process-id: 789c9262-5dd3-4d58-a824-c2a099892bd8
          desc: after patching resources with preserve annotation

      - run:
          name: Verify resources were not patched back by the upgrader
          command: |
            resources="$(kubectl -n stackrox get deploy/sensor -o 'jsonpath=cpu={.spec.template.spec.containers[?(@.name=="sensor")].resources.requests.cpu},memory={.spec.template.spec.containers[?(@.name=="sensor")].resources.requests.memory}')"
            if [[ "$resources" != 'cpu=1100m,memory=1181116006400m' ]]; then
              echo "Resources ($resources) appear patched back!"
              kubectl -n stackrox get deploy/sensor -o yaml
              exit 1
            fi
            toleration="$(kubectl -n stackrox get deploy/sensor -o json | jq -rc '.spec.template.spec.tolerations[0] | (.effect + "," + .key + "," + .operator + "," + .value)')"
            echo "Found toleration: $toleration"
            if [[ "$toleration" != 'NoSchedule,thekey,Equal,thevalue' ]]; then
              echo "Tolerations were not passed through to new Sensor"
              kubectl -n stackrox get deploy/sensor -o yaml
              exit 1
            fi

      - remove-existing-stackrox-resources

      - run:
          name: Deploy Central version right after 41.4
          command: |
            git checkout 309f7d8edc03e9767ddabefad670224e4bd1746f
            # We explicitly need to set CIRCLE_TAG to empty, as it will otherwise dictate
            # the output of `make tag`.
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag CIRCLE_TAG=)"
            ./deploy/k8s/central.sh

            # Remove the scanner HPA for these tests to avoid resource throttling
            kubectl -n stackrox delete hpa/scanner

            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - *waitForAPI

      - run:
          name: Deploy and restore DB to version 41.4
          command: |
            # Restore 41.4 backup to 41.4 based commit
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m 41.4-upgrade-db.zip

      - *waitForAPI
      - run:
          name: Checkout current commit again
          command: git reset --hard "${CIRCLE_SHA1}"

      - run:
          name: Roll up to a 44.x image so that RocksDB is available
          command: |
            kubectl -n stackrox set image deploy/central *=stackrox/main:3.0.44.x-7-g633d112298

      - *waitForAPI

      - run:
          name: Take backup of BadgerDB on 44
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db backup --output badgerdb-44-backup.zip

      - run:
          name: Set DB to RocksDB
          command: |
            kubectl -n stackrox set env deploy/central ROX_ROCKSDB=true
      - check-stackrox-logs:
          collect: true
      - *waitForAPI

      - run:
          name: Take backup of RocksDB on 44
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db backup --output rocksdb-44-backup.zip

      - run:
          name: Set Central image to current SHA
          command: |
            kubectl -n stackrox set image deploy/central "central=stackrox/main:$(make --quiet tag)"
      - *waitForAPI
      - validate-upgrade:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Validate roll forward to RocksDB from RocksDB on 44"
      - check-stackrox-logs:
          collect: true

      - run:
          name: Restore 41.4 backup to current head
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m 41.4-upgrade-db.zip

      - *waitForAPI
      - validate-upgrade-and-check-logs-with-bouncing:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Validate restore from 41.4 to current head"

      - run:
          name: Restore 44 BadgerDB backup to current head
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m badgerdb-44-backup.zip

      - *waitForAPI
      - validate-upgrade-and-check-logs-with-bouncing:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Validate 44 restore to RocksDB from BadgerDB"

      - run:
          name: Restore 44 RocksDB backup to current head
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore --timeout 2m rocksdb-44-backup.zip

      - *waitForAPI
      - validate-upgrade-and-check-logs-with-bouncing:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Validate 44 restore to RocksDB from RocksDB"

      - run:
          name: "Backup RocksDB"
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db backup --output rocksdb-backup.zip

      - run:
          name: "Restore RocksDB"
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore rocksdb-backup.zip

      - *waitForAPI
      - validate-upgrade-and-check-logs-with-bouncing:
          upgrade-cluster-id: "ff725187-e4f3-4d10-b5bd-021fc19bf108"
          desc: "Validate RocksDB after backup + restore"

      - run:
          name: Fetch bundle for the "remote" cluster in the DB
          command: |
            roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" sensor get-bundle remote
            [[ -d sensor-remote ]]

      - run:
          name: Install the sensor
          command: ./sensor-remote/sensor.sh

      - *waitForSensorK8s

      - run:
          name: Wait for Central reconciliation before running smoke tests
          command: |
            # Reconciliation is rather slow in this case, since the central has a DB with a bunch of deployments,
            # none of which exist. So when sensor connects, the reconciliation deletion takes a while to flush.
            # This causes flakiness with the smoke tests.
            # To mitigate this, wait for the deployments to get deleted before running the tests.
            success=0
            for i in $(seq 1 90); do
              numDeployments="$(curl -sSk -u "admin:${ROX_PASSWORD}" "https://${API_ENDPOINT}/v1/summary/counts" | jq '.numDeployments' -r)"
              echo "Try number ${i}. Number of deployments in Central: ${numDeployments}"
              [[ -n "${numDeployments}" ]]
              if [[ "${numDeployments}" -lt 100 ]]; then
                success=1
                break
              fi
              sleep 10
            done
            [[ "${success}" == 1 ]]

      - run:
          name: Run smoke tests now that sensor is up
          command: |
            export CLUSTER=K8S
            make -C qa-tests-backend smoke-test

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *storeK8sLogs
      - *storeQALogs

      - teardown-gke

  gke-api-scale-tests:
    executor: custom
    environment:
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - STORAGE: pvc
      - STORAGE_CLASS: faster
      - STORAGE_SIZE: 100
      - LOAD_BALANCER: lb
      - OUTPUT_FORMAT: helm

    steps:
      - checkout
      - check-backend-changes
      - check-label-exists-or-skip:
          label: ci-scale-tests
          run-on-master: false
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - setup-dep-env
      - attach-gke-cluster:
          cluster-id: scale-test
      - remove-existing-stackrox-resources

      - *setupGoogleAppCreds
      - *setupLicense

      - run:
          name: Launch Sensor and Central with scale workload
          command: |
            ./deploy/k8s/deploy.sh
            ./scale/launch_workload.sh scale-test

            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - *waitForAPI

      - run:
          name: Collect scale results
          command: |
            mkdir /tmp/pprof
            # 45 min run so that we are confident that the run has completely finished
            ./scale/profiler/pprof.sh /tmp/pprof "${API_ENDPOINT}" 45
            zip -r /tmp/pprof.zip /tmp/pprof

      - *storeProfilingResults
      - *connectToMonitoring
      - *collectMonitoringImages
      - *storeMonitoringImages
      - *storeMonitoringMetrics
      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data

      - *getPrometheusMetricParser
      - compare-with-stored-metrics:
          cluster-flavor: gke
          test: scale-test
      - run:
          name: Store metrics
          command: |
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              THIS_RUN=$(echo debug-dump/stackrox_debug*.zip)
              GS_PATH=gs://stackrox-ci-metrics/scale-test/gke/
              gsutil cp "${THIS_RUN}" "${GS_PATH}"
              unzip -d debug-dump/stackrox_debug "${THIS_RUN}"
              prometheus-metric-parser single --file=debug-dump/stackrox_debug/metrics-2 \
                --format=gcp-monitoring --labels='Test=ci-scale-test,ClusterFlavor=gke' \
                --project-id=stackrox-ci --timestamp=$(date -u +"%s")
            fi

      - check-stackrox-logs
      - *storeK8sLogs

      - teardown-gke

  openshift-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ROX_SENSOR_DETECTION: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - run-openshift-tests:
          gate-label: ci-openshift-tests
          version: 0.1.1

  openshift-crio-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ROX_SENSOR_DETECTION: true
      - ROX_SUPPORT_SLIM_COLLECTOR_MODE: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_CENTRAL_INSTALLATION_EXPERIENCE: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - run-openshift-tests:
          suffix: "-crio"
          gate-label: ci-openshift-crio-tests
          version: 0.1.1

  eks-api-e2e-tests:
    executor: custom
    environment:
      - EKS_AUTOMATION_VERSION: 0.1.5
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_DETECTION: true

    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-eks-tests
          run-on-master: false

      - setup_remote_docker

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Sanity check KUBECONFIG
          command: |
            ls -lh $PWD/eks/data
            export KUBECONFIG=$PWD/eks/data/eks-kube.yaml
            kubectl get nodes

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/eks/data/eks-kube.yaml"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.4
            cci-export ROX_IMAGE_REGISTRY docker.io
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-webhook-server
      - deploy-authz-plugin

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform
          command: |
            export CLUSTER=K8S
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            make -C qa-tests-backend test

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: eks-tests

      - *updateTestRail

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs
      - *storeQALogs

      - *loginToGCR
      - run:
          name: Destroy EKS cluster
          command: |
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: always

  kops-api-e2e-tests:
    executor: custom
    environment:
      - KOPS_AUTOMATION_VERSION: 0.0.9
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true

    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-kops-tests
          run-on-master: false

      - setup_remote_docker

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Add /etc/hosts entry
          command: echo "$(cat $PWD/kops/master_ip) api.$(cat $PWD/kops/cluster_name)" | sudo tee -a /etc/hosts

      - run:
          name: Sanity check KUBECONFIG
          command: |
            ls -lh $PWD/kops
            export KUBECONFIG=$PWD/kops/kube.yaml
            kubectl get nodes

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/kops/kube.yaml"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.4
            cci-export KOPS_HOST "$(cat kops/master)"
            cci-export ROX_IMAGE_REGISTRY docker.io
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-webhook-server
      - deploy-authz-plugin

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform
          command: |
            export CLUSTER=K8S
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            make -C qa-tests-backend test

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: kops-tests

      - *updateTestRail

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs
      - *storeQALogs

      - run:
          name: Pack kops volume
          command: |
            docker create -v /data --name kops alpine:3.9 /bin/true
            docker cp kops/id_rsa            kops:/data
            docker cp kops/id_rsa.pub        kops:/data
            docker cp kops/terraform.tfstate kops:/data
            docker cp kops/cluster_name      kops:/data
            docker cp kops/kops_s3_bucket    kops:/data
            docker cp kops/delete-cluster-$(cat kops/cluster_name).sh kops:/data
          when: always

      - *loginToGCR
      - run:
          name: Destroy Kops cluster
          command: |
            docker run --rm -t \
              --volumes-from kops \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/kops:$KOPS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: always

  roxctl-windows-test:
    executor:
      name: win/default  # References orbs.win defined above.
      size: "medium"

    steps:
      - checkout
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: "Generate K8s/OpenShift bundles using roxctl.exe"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            ./bin/windows/roxctl.exe central generate k8s none --output-dir rox-k8s-bundle
            ./bin/windows/roxctl.exe central generate openshift none --output-dir rox-openshift-bundle

      - run:
          name: "Verify contents of generated bundles"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            dir --recursive rox-k8s-bundle
            test -f rox-k8s-bundle/README
            dir --recursive rox-openshift-bundle
            test -f rox-openshift-bundle/README

      - run:
          name: "Verify version"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            version_from_yaml="$(grep -rnw -e 'image.*main' rox-k8s-bundle/central | sed -E 's/.*main:([^" ]+).*/\1/g')"
            version_from_roxctl="$(./bin/windows/roxctl.exe version)"
            echo "Versions: from YAML ${version_from_yaml}; from roxctl ${version_from_roxctl}"
            test "${version_from_yaml}" = "${version_from_roxctl}"

  ###
  # Post-build jobs to handle built artifacts (like tag, scan and publish images)
  ###

  mark-collector-release:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints:
            - "7f:08:58:1e:80:80:6e:66:99:9a:37:cb:e9:96:0b:40"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Check out collector source code
          command: |
            mkdir -p /tmp/collector
            cd /tmp/collector
            git clone git@github.com:stackrox/collector.git .

      - run:
          name: Add current release version to RELEASED_VERSIONS file
          command: |
            collector_version="$(cat COLLECTOR_VERSION)"
            cd /tmp/collector
            git checkout master && git pull

            # We need to make sure the file ends with a newline so as not to corrupt it when appending.
            [[ ! -f RELEASED_VERSIONS ]] || sed -i'' -e '$a\' RELEASED_VERSIONS

            echo "${collector_version} ${CIRCLE_TAG}  # Rox release ${CIRCLE_TAG} by ${CIRCLE_USERNAME} at $(date)" \
              >>RELEASED_VERSIONS
            git add RELEASED_VERSIONS
            git -c "user.name=roxbot" -c "user.email=roxbot@stackrox.com" commit \
                -m "Automatic update of RELEASED_VERSIONS file for Rox release ${CIRCLE_TAG}"
            git push

  push-release:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Retag docs image with release-version tag in Docker Hub
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            DOCS_TAG=$(make --quiet docs-tag)
            MAIN_TAG=$(make --quiet tag)

            # The docs image uses the same tag as main for releases,
            # but is built in github.com/stackrox/docs with different tags.
            # This step places it in Docker Hub with the release tag
            # for convenience before running the stackrox.io push.
            docker pull docker.io/stackrox/docs:$DOCS_TAG | cat
            docker tag  docker.io/stackrox/docs:$DOCS_TAG docker.io/stackrox/docs:$MAIN_TAG
            docker push docker.io/stackrox/docs:$MAIN_TAG | cat

      - run:
          name: Push image into stackrox.io
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            docker login -u "$STACKROX_IO_PUSH_USERNAME" -p "$STACKROX_IO_PUSH_PASSWORD" stackrox.io
            docker login -u "$STACKROX_IO_PUSH_USERNAME" -p "$STACKROX_IO_PUSH_PASSWORD" collector.stackrox.io

            MAIN_TAG=$(make --quiet tag)
            COLLECTOR_TAG=$(make --quiet collector-tag)

            docker pull docker.io/stackrox/main:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/main:$MAIN_TAG stackrox.io/main:$MAIN_TAG
            docker push        stackrox.io/main:$MAIN_TAG | cat

            # The docs image uses the same tag as main for releases.
            docker pull docker.io/stackrox/docs:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/docs:$MAIN_TAG stackrox.io/docs:$MAIN_TAG
            docker push        stackrox.io/docs:$MAIN_TAG | cat

            docker pull docker.io/stackrox/roxctl:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/roxctl:$MAIN_TAG stackrox.io/roxctl:$MAIN_TAG
            docker push        stackrox.io/roxctl:$MAIN_TAG | cat

            docker pull    docker.io/stackrox/collector:$COLLECTOR_TAG | cat
            docker pull    "docker.io/stackrox/collector:${COLLECTOR_TAG}-latest" | cat
            docker pull    "docker.io/stackrox/collector:${COLLECTOR_TAG}-base" | cat
            docker tag     docker.io/stackrox/collector:$COLLECTOR_TAG collector.stackrox.io/collector:$COLLECTOR_TAG
            docker tag     "docker.io/stackrox/collector:${COLLECTOR_TAG}-latest" "collector.stackrox.io/collector:${COLLECTOR_TAG}-latest"
            docker tag     "docker.io/stackrox/collector:${COLLECTOR_TAG}-base" "collector.stackrox.io/collector:${COLLECTOR_TAG}-slim"
            docker push collector.stackrox.io/collector:$COLLECTOR_TAG | cat
            docker push "collector.stackrox.io/collector:${COLLECTOR_TAG}-latest" | cat
            docker push "collector.stackrox.io/collector:${COLLECTOR_TAG}-slim" | cat

            # RHEL images
            docker pull docker.io/stackrox/main-rhel:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/main-rhel:$MAIN_TAG stackrox.io/main-rhel:$MAIN_TAG
            docker push        stackrox.io/main-rhel:$MAIN_TAG | cat

            docker pull    docker.io/stackrox/collector-rhel:$COLLECTOR_TAG | cat
            docker pull    "docker.io/stackrox/collector-rhel:${COLLECTOR_TAG}-latest" | cat
            docker pull    "docker.io/stackrox/collector-rhel:${COLLECTOR_TAG}-base" | cat
            docker tag     docker.io/stackrox/collector-rhel:$COLLECTOR_TAG collector.stackrox.io/collector-rhel:$COLLECTOR_TAG
            docker tag     "docker.io/stackrox/collector-rhel:${COLLECTOR_TAG}-latest" "collector.stackrox.io/collector-rhel:${COLLECTOR_TAG}-latest"
            docker tag     "docker.io/stackrox/collector-rhel:${COLLECTOR_TAG}-base" "collector.stackrox.io/collector-rhel:${COLLECTOR_TAG}-slim"
            docker push collector.stackrox.io/collector-rhel:$COLLECTOR_TAG | cat
            docker push "collector.stackrox.io/collector-rhel:${COLLECTOR_TAG}-latest" | cat
            docker push "collector.stackrox.io/collector-rhel:${COLLECTOR_TAG}-slim" | cat

      - run:
          name: Push roxctl to stackrox-hub
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list
            MAIN_TAG=$(make --quiet tag)

            cd bin
            for release in "${MAIN_TAG}" latest; do
              for platform in Linux Darwin Windows; do
                platform_lower="$(echo "$platform" | tr A-Z a-z)"
                roxctl_bin="roxctl"
                if [[ "${platform}" == "Windows" ]]; then
                  roxctl_bin="roxctl.exe"
                fi
                gsutil cp "${platform_lower}/${roxctl_bin}" "gs://sr-roxc/${release}/bin/${platform}/${roxctl_bin}"
                gsutil cp "${platform_lower}/${roxctl_bin}" "gs://sr-roxc/${release}/bin/${platform_lower}/${roxctl_bin}"
              done
            done

      - run:
          name: Push image into gcr.io for GCP Marketplace
          command: |
            docker login -u _json_key -p "$GOOGLE_CREDENTIALS_GCR_SCANNER" https://gcr.io

            MAIN_TAG=$(make --quiet tag)
            SCANNER_TAG=$(make --quiet scanner-tag)

            # Deployer was pushed up in build-deployer, so must be pulled back down here.
            docker pull docker.io/stackrox/deployer:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/deployer:$MAIN_TAG gcr.io/stackrox-launcher-project-1/stackrox/deployer:$MAIN_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/deployer:$MAIN_TAG | cat

            # Main was pushed up in the previous step, so just needs to be tagged.
            docker tag  stackrox.io/main:$MAIN_TAG gcr.io/stackrox-launcher-project-1/stackrox:$MAIN_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox:$MAIN_TAG | cat

            # Scanner was pushed up in its own repo, so must be pulled back down here.
            docker pull stackrox.io/scanner:$SCANNER_TAG | cat
            docker tag  stackrox.io/scanner:$SCANNER_TAG gcr.io/stackrox-launcher-project-1/stackrox/scanner:$SCANNER_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/scanner:$SCANNER_TAG | cat

            # Scanner DB was pushed up in its own repo, so must be pulled back down here.
            docker pull stackrox.io/scanner-db:$SCANNER_TAG | cat
            docker tag  stackrox.io/scanner-db:$SCANNER_TAG gcr.io/stackrox-launcher-project-1/stackrox/scanner-db:$SCANNER_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/scanner-db:$SCANNER_TAG | cat

      - run:
          name: Push offline image bundles to stackrox.io
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list

            make offline-bundle

            for release in "$(make --quiet tag)" latest; do
              gsutil cp scripts/offline-bundle/image-bundle.tgz           gs://sr-roxc/${release}/image-bundle.tgz
              gsutil cp scripts/offline-bundle/image-collector-bundle.tgz gs://sr-roxc/${release}/image-collector-bundle.tgz
              gsutil cp scripts/offline-bundle/image-bundle-rhel.tgz           gs://sr-roxc/${release}/image-bundle-rhel.tgz
              gsutil cp scripts/offline-bundle/image-collector-bundle-rhel.tgz gs://sr-roxc/${release}/image-collector-bundle-rhel.tgz
            done

      - run:
          name: Upload main image RHEL DSOP bundle to dsop-artifacts.stackrox.io
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list
            TAG="$(make --quiet tag)"
            gsutil cp image/rhel/bundle.tar.gz "gs://dsop-artifacts.stackrox.io/main-rhel/${TAG}/"

      - run:
          name: Publish Helm charts to github repository stackrox/release-artifacts and create a PR
          command: |
            MAIN_TAG=$(make --quiet tag)
            make render-helm-yamls
            sensor_chart_dir="/tmp/${MAIN_TAG}"
            central_chart_dir="$(mktemp -d)"
            rmdir "$central_chart_dir"
            "./bin/$(uname | tr A-Z a-z)/roxctl" helm output central-services --output-dir "${central_chart_dir}"
            .circleci/publish-helm-charts "$MAIN_TAG" "${central_chart_dir}" "${sensor_chart_dir}"
            rm -rf "${sensor_chart_dir}"

      - run:
          name: Push image into ECR for AWS Marketplace
          command: |
            ECR_REGISTRY="226581168482.dkr.ecr.us-east-1.amazonaws.com/stackrox"
            aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin "$ECR_REGISTRY"

            MAIN_TAG=$(make --quiet tag)
            SCANNER_TAG=$(make --quiet scanner-tag)
            COLLECTOR_TAG=$(make --quiet collector-tag)

            # Main was pushed up in a previous step, so just needs to be tagged.
            docker tag  "stackrox.io/main:$MAIN_TAG" "$ECR_REGISTRY/main:$MAIN_TAG"
            docker push "$ECR_REGISTRY/main:$MAIN_TAG" | cat

            # Scanner was pushed up in its own repo, so must be pulled back down here.
            docker pull "stackrox.io/scanner:$SCANNER_TAG" | cat
            docker tag  "stackrox.io/scanner:$SCANNER_TAG" "$ECR_REGISTRY/scanner:$SCANNER_TAG"
            docker push "$ECR_REGISTRY/scanner:$SCANNER_TAG" | cat

            # Scanner-db was pushed up in its own repo, so must be pulled back down here.
            docker pull "stackrox.io/scanner-db:$SCANNER_TAG" | cat
            docker tag  "stackrox.io/scanner-db:$SCANNER_TAG" "$ECR_REGISTRY/scanner-db:$SCANNER_TAG"
            docker push "$ECR_REGISTRY/scanner-db:$SCANNER_TAG" | cat

            # Docs was pushed up in its own repo, so must be pulled back down here.
            docker pull "stackrox.io/docs:$MAIN_TAG" | cat
            docker tag  "stackrox.io/docs:$MAIN_TAG" "$ECR_REGISTRY/docs:$MAIN_TAG"
            docker push "$ECR_REGISTRY/docs:$MAIN_TAG" | cat

            # Collector was pushed up in its own repo, so must be pulled back down here.
            docker pull "collector.stackrox.io/collector:$COLLECTOR_TAG" | cat
            docker tag  "collector.stackrox.io/collector:$COLLECTOR_TAG" "$ECR_REGISTRY/collector:$COLLECTOR_TAG"
            docker push "$ECR_REGISTRY/collector:$COLLECTOR_TAG" | cat

            echo ""
            echo "Images pushed to Marketplace ECR:"
            echo "MAIN:       $ECR_REGISTRY/main:$MAIN_TAG"
            echo "SCANNER:    $ECR_REGISTRY/scanner:$SCANNER_TAG"
            echo "SCANNER-DB: $ECR_REGISTRY/scanner-db:$SCANNER_TAG"
            echo "COLLECTOR:  $ECR_REGISTRY/collector:$COLLECTOR_TAG"
            echo "DOCS:       $ECR_REGISTRY/docs:$DOCS_TAG"

  scan-images-in-quay:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Push images into quay.io
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            docker login -u "$QUAY_USERNAME" -p "$QUAY_PASSWORD" quay.io
            ./release/scripts/vuln_scan.sh

      - run:
          name: Check for fixable vulns
          command: |
            export QUAY_BEARER_TOKEN=${QUAY_BEARER_TOKEN}
            ./release/scripts/vuln_check.sh

  test-helm-charts-repo:
    executor: custom
    environment:
      - SCANNER_SUPPORT: true
      - AUTH0_SUPPORT: false

    steps:
      - checkout
      - check-backend-changes

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup_remote_docker
      - setup-gcp
      - setup-dep-env

      - attach-gke-cluster:
          cluster-id: test-helm-charts

      - *setupGoogleAppCreds
      - run:
          name: Creating License Key
          command: |
            echo "Issuing a CI license for testing the release build"
            ROX_LICENSE_KEY="$(licenses/ci.sh)"
            cci-export ROX_LICENSE_KEY "$ROX_LICENSE_KEY"

      - deploy-stackrox:
          sensor-deploy-flavor: helm
          helm-from-repo: true
          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - *collectK8sLogs
      - get-and-store-debug-dump
      - pull-stackrox-api-data
      - *backupDB
      - *storeDBBackupArtifact
      - check-stackrox-logs
      - *storeK8sLogs
      - teardown-gke

  ###
  # Infrastructure and workflow utility jobs
  ###

  retrigger-master-workflow:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - run:
          name: Maybe retrigger master workflow
          command: |
            [[ -n "${CIRCLE_TAG}" ]] # Assert that this job is only run on tags
            # If the tag was pushed on master, then retrigger the workflow!
            # We do this because the output of `make tag` changes in this case, and we want to rerun the master workflow
            # in order to produce new images with the updated version of `make tag`.
            # Note that Circle changes the local master ref to point to the tag, so we need to check against origin/master.
            if git branch -a --contains "${CIRCLE_TAG}" | grep -qx '[[:space:]]*remotes/origin/master$'; then
              curl -sS --fail -X POST -u "${CIRCLE_TOKEN_ROXBOT}:" "https://circleci.com/api/v1.1/project/github/stackrox/rox/build" -d '{"branch": "master"}'
            fi

  regenerate-dev-license-if-required:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox
      - *setupRoxctl

      - run:
          name: Halt unless license is expiring soon
          command: |
            license_expiry_epoch="$(roxctl central license info --license=deploy/common/dev-license.lic --json | jq '.restrictions.notValidAfter' -r | sed 's/\.[0-9]*//' | jq -R 'fromdateiso8601')"
            [[ -n "${license_expiry_epoch}" ]]
            echo "License expiry timestamp: ${license_expiry_epoch}"
            now="$(date +%s)"
            days_until_expiry="$(( (license_expiry_epoch-now)/(24*60*60) ))"
            echo "Days until expiry: ${days_until_expiry}"
            if [[ "${days_until_expiry}" -ge 15 ]]; then
              circleci step halt
            fi

      - run:
          name: Halt if there is an existing branch with the dev license.
          command: |
            if git branch -a | grep -qx '[[:space:]]*remotes/origin/roxbot/regenerate-dev-license'; then
              echo "Found existing branch, halting..."
              circleci step halt
            fi

      - *setupGoogleAppCreds
      - run:
          name: Regenerate development license
          command: |
            scripts/regenerate-dev-license.sh

      - run:
          name: Configure git
          command: |
            git config user.email "roxbot@stackrox.com"
            git config user.name "RoxBot"

      - add_ssh_keys:
          fingerprints:
            - "b0:90:e9:6f:77:d5:fe:b6:8c:1a:a5:3f:a4:e3:41:e5"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Create temporary branch with license update commit
          command: |
            git add deploy/common/dev-license.lic
            git checkout -b "roxbot-tmp/regenerate-dev-license"
            git commit \
              -m"Automatic re-generation of development license ($(date '+%Y-%m-%d'))"

      - run:
          name: Push updated dev license to remote
          command: |
            commit="$(git rev-parse "roxbot-tmp/regenerate-dev-license")"

            git checkout master

            for i in $(seq 1 3); do
              echo "Attempting to merge into master (attempt $i of 3) ..."

              git fetch && git reset --hard origin/master
              # Note that the above `git reset --hard` requires a clean working tree. We therefore
              # cherry-pick the license update commit instead of generating it now such that we
              # invoke all scripts from the current branch (mostly relevant for testing purposes).
              git cherry-pick "$commit"
              if git push origin master:"roxbot/regenerate-dev-license"; then
                echo "Success!"
                exit 0
              fi

              echo "Push did not succeed. Sleeping for 10 seconds, then trying again."
              sleep 10
            done

            echo "Failed to merge into master."
            exit 1

      - run:
          name: Create PR
          command: |
            git checkout "$CIRCLE_BRANCH"
            scripts/ci/create-dev-license-update-pr.sh "roxbot/regenerate-dev-license"

  trigger-nightly-build:
    executor: custom
    resource_class: small
    steps:
      - checkout

      - run:
          name: Configure git
          command: |
            git config user.email "roxbot@stackrox.com"
            git config user.name "RoxBot"

      - add_ssh_keys:
          fingerprints:
            - "b0:90:e9:6f:77:d5:fe:b6:8c:1a:a5:3f:a4:e3:41:e5"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Create a commit and tag for nightly build
          command: |
            # Add an empty commit to diverge from master
            git commit --allow-empty -m "Nightly build $(date)"
            NIGHTLY_TAG="$(git describe --tags --abbrev=0)-nightly-$(date '+%Y%m%d')"
            git tag "$NIGHTLY_TAG"
            cci-export NIGHTLY_TAG "$NIGHTLY_TAG"

      - run:
          name: Push nightly tag to remote
          command: |
            git push origin "$NIGHTLY_TAG"

  collect-test-jobs-stats:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - setup-gcp

      - run:
          name: Wait for all other workflow jobs to finish
          command: CIRCLE_TOKEN="$CIRCLE_TOKEN_ROXBOT" .circleci/wait-for-jobs-completion.sh

      - run:
          name: Collect and send test jobs stats
          command: CIRCLE_TOKEN="$CIRCLE_TOKEN_ROXBOT" .circleci/collect-tests-stats.sh "stackrox-ci" "ci_metrics.rox_jobs_stats"

workflows:
  version: 2

  build_all:
    when: 
      not: << pipeline.parameters.run_designated_image_test >>
    jobs:
      - slack-notify:
          filters:
            tags:
              only: /.*/
            branches:
              ignore: /.*/
          context: docker-io-pull
      - style-checks:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - check-generated-files-up-to-date:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - unit-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - integration-unit-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - pre-build-ui:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - pre-build-cli:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - pre-build-go-binaries:
          <<: *runOnAllTagsWithDockerIOPushCtx
      - build:
          <<: *runOnAllTagsWithDockerIOPushCtx
          requires:
            - pre-build-ui
            - pre-build-cli
            - pre-build-go-binaries
      - build-rhel:
          <<: *runOnAllTagsWithDockerIOPushCtx
      - build-deployer:
          <<: *runOnAllTagsWithDockerIOPushCtx
          requires:
            - pre-build-cli
      - build-scale-monitoring-and-mock-server:
          <<: *runOnAllTagsWithDockerIOPushCtx
      - provision-gke-api-nongroovy-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-api-nongroovy-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-api-nongroovy-tests
      - provision-gke-ui-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-ui-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-ui-e2e-tests
      - provision-gke-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-api-e2e-tests
      - provision-gke-kernel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-kernel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-kernel-api-e2e-tests
      - provision-gke-api-upgrade-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-api-upgrade-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-api-upgrade-tests
      - provision-gke-api-scale-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-api-scale-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-gke-api-scale-tests
      - provision-openshift-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - openshift-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-openshift-api-e2e-tests
      - provision-openshift-crio-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - openshift-crio-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-openshift-crio-api-e2e-tests
      - provision-eks-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - eks-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-eks-api-e2e-tests
      - provision-kops-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - kops-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build
            - build-scale-monitoring-and-mock-server
            - provision-kops-api-e2e-tests
      - roxctl-windows-test:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - pre-build-cli
      - mark-collector-release:
          <<: *runOnlyOnReleaseBuilds
          context: docker-io-pull
          requires:
            - build
            - unit-tests
            - push-release
      - provision-openshift-rhel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - openshift-rhel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-openshift-rhel-api-e2e-tests
            - pre-build-cli
            - pre-build-go-binaries
      - provision-gke-rhel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
      - gke-rhel-api-e2e-tests:
          <<: *runOnAllTagsWithDockerIOPullCtx
          requires:
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-gke-rhel-api-e2e-tests
            - pre-build-cli
            - pre-build-go-binaries

      - push-release:
          <<: *runOnlyOnReleaseBuilds
          context: docker-io-and-stackrox-io-push
          requires:
            - build
            - build-rhel
            - build-deployer
            - unit-tests

      - scan-images-in-quay:
          <<: *runOnlyOnReleaseCandidateBuilds
          context: docker-io-and-stackrox-io-push
          requires:
            - build
            - build-rhel

      - retrigger-master-workflow:
          context: docker-io-pull
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /.*/

      - collect-test-jobs-stats:
          context: docker-io-pull
          filters:
            branches:
              only: master
            tags:
              only: /.*-nightly-.*/

      - provision-test-helm-charts-repo:
          <<: *runOnlyOnReleaseBuilds
          context: docker-io-pull
      - test-helm-charts-repo:
          <<: *runOnlyOnReleaseBuilds
          context: docker-io-pull
          requires:
            - push-release
            - provision-test-helm-charts-repo

  update_dev_license_if_required:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only: master

    jobs:
      - pre-build-cli:
          context: docker-io-pull
      - regenerate-dev-license-if-required:
          context: docker-io-pull
          requires:
            - pre-build-cli

  nightly:
    triggers:
      - schedule:
          cron: "0 9 * * *"
          filters:
            branches:
              only: master

    jobs:
      - trigger-nightly-build:
          context: docker-io-pull
          filters:
            branches:
              only: master

  designated_image_test:
    when: << pipeline.parameters.run_designated_image_test >>
    jobs:
      - designated-image-tests:
          context: docker-io-pull
