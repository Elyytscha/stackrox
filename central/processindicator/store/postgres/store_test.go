// Code generated by pg-bindings generator. DO NOT EDIT.

//go:build sql_integration

package postgres

import (
	"context"
	"encoding/csv"
	"os"
	"strconv"
	"testing"
	"time"

	"github.com/jackc/pgx/v4/pgxpool"
	"github.com/stackrox/rox/generated/storage"
	"github.com/stackrox/rox/pkg/features"
	"github.com/stackrox/rox/pkg/fixtures"
	"github.com/stackrox/rox/pkg/postgres/pgtest"
	"github.com/stackrox/rox/pkg/testutils/envisolator"
	"github.com/stackrox/rox/pkg/uuid"
	"github.com/stretchr/testify/suite"
)

type ProcessIndicatorsStoreSuite struct {
	suite.Suite
	envIsolator *envisolator.EnvIsolator
}

func TestProcessIndicatorsStore(t *testing.T) {
	suite.Run(t, new(ProcessIndicatorsStoreSuite))
}

func (s *ProcessIndicatorsStoreSuite) SetupTest() {
	s.envIsolator = envisolator.NewEnvIsolator(s.T())
	s.envIsolator.Setenv(features.PostgresDatastore.EnvVar(), "true")

	if !features.PostgresDatastore.Enabled() {
		s.T().Skip("Skip postgres store tests")
		s.T().SkipNow()
	}
}

func (s *ProcessIndicatorsStoreSuite) TearDownTest() {
	s.envIsolator.RestoreAll()
}

func (s *ProcessIndicatorsStoreSuite) TestStore() {
	f, err := os.Create("results.csv")
	defer f.Close()

	if err != nil {

		log.Errorf("failed to open file", err)
	}

	w := csv.NewWriter(f)

	source := pgtest.GetConnectionString(s.T())
	config, err := pgxpool.ParseConfig(source)
	if err != nil {
		panic(err)
	}
	pool, err := pgxpool.ConnectConfig(context.Background(), config)
	s.NoError(err)
	defer pool.Close()

	Destroy(pool)
	store := New(pool)

	processIndicator := fixtures.GetProcessIndicator()
	foundProcessIndicator, exists, err := store.Get(processIndicator.GetId())
	s.NoError(err)
	s.False(exists)
	s.Nil(foundProcessIndicator)

	s.NoError(store.Upsert(processIndicator))
	foundProcessIndicator, exists, err = store.Get(processIndicator.GetId())
	s.NoError(err)
	s.True(exists)
	s.Equal(processIndicator, foundProcessIndicator)

	processIndicatorCount, err := store.Count()
	s.NoError(err)
	s.Equal(processIndicatorCount, 1)

	processIndicatorExists, err := store.Exists(processIndicator.GetId())
	s.NoError(err)
	s.True(processIndicatorExists)
	s.NoError(store.Upsert(processIndicator))

	foundProcessIndicator, exists, err = store.Get(processIndicator.GetId())
	s.NoError(err)
	s.True(exists)
	s.Equal(processIndicator, foundProcessIndicator)

	s.NoError(store.Delete(processIndicator.GetId()))
	foundProcessIndicator, exists, err = store.Get(processIndicator.GetId())
	s.NoError(err)
	s.False(exists)
	s.Nil(foundProcessIndicator)

	//batchSize := 100
	//batches := []int{100,250,500,1000,5000,10000,20000}
	batches := []int{100,250,500,1000,5000,10000,20000}
	results := [][]string{}
	numRecords := 1000
	var indicators []*storage.ProcessIndicator
	for i := 0; i < numRecords; i++ {
		pi := fixtures.GetProcessIndicator()
		pi.Id = uuid.NewV4().String()
		pi.PodId = strconv.Itoa(i)
		indicators = append(indicators, pi)
	}

	for _, batchSize := range batches {
		result := []string{}
		result = append(result, strconv.Itoa(batchSize))

		indicators = nil
		for i := 0; i < numRecords; i++ {
			pi := fixtures.GetProcessIndicator()
			pi.Id = uuid.NewV4().String()
			pi.PodId = strconv.Itoa(i)
			indicators = append(indicators, pi)
		}

		log.Info("One at a time")
		a := time.Now()
		s.NoError(store.UpsertMany(indicators))

		delta := time.Now().Sub(a)
		result = append(result, strconv.FormatInt(delta.Milliseconds(), 10))
		log.Infof("%d\n", delta.Milliseconds())

		log.Info("Multi  Value")
		if batchSize <= 1000 {

			indicators = nil
			for i := 0; i < numRecords; i++ {
				pi := fixtures.GetProcessIndicator()
				pi.Id = uuid.NewV4().String()
				pi.PodId = strconv.Itoa(i)
				indicators = append(indicators, pi)
			}

			a = time.Now()
			s.NoError(store.UpsertManyMultiVal(indicators, batchSize))

			delta = time.Now().Sub(a)
			result = append(result, strconv.FormatInt(delta.Milliseconds(), 10))
		} else {
			result = append(result, "N/A")
		}
		log.Infof("%d\n", delta.Milliseconds())

		processIndicatorCount, err = store.Count()
		log.Infof("Indicator counts = %d", processIndicatorCount)

		log.Infof("Copy with batch %d", batchSize)
		indicators = nil
		for i := 0; i < numRecords; i++ {
			pi := fixtures.GetProcessIndicator()
			pi.Id = uuid.NewV4().String()
			pi.PodId = strconv.Itoa(i)
			indicators = append(indicators, pi)
		}
		a = time.Now()
		s.NoError(store.UpsertManyPGCopy(indicators, batchSize))
		//s.NoError(store.UpsertManyMultiVal(indicators, batchSize))
		delta = time.Now().Sub(a)
		result = append(result, strconv.FormatInt(delta.Milliseconds(), 10))
		log.Infof("%d\n", delta.Milliseconds())

		processIndicatorCount, err = store.Count()
		log.Infof("Indicator counts = %d", processIndicatorCount)

		results = append(results, result)
	}

	log.Info(results)
	err = w.WriteAll(results) // calls Flush internally
	log.Info("Successfully loaded the DB")

}
